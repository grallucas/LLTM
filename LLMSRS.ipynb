{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SRS import SRS\n",
    "from llama_cpp import Llama\n",
    "\n",
    "def pretty_print_history(currently_generating):\n",
    "    hist = ''\n",
    "    for msg in history+[currently_generating]:\n",
    "        hist += msg['role'] + ' -- ' + msg['content'] + '\\n'\n",
    "    return hist\n",
    "\n",
    "\n",
    "from IPython.display import clear_output # for clearing the output\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fsrs.models.Card object at 0x7f9a60201100>\n",
      "0\n",
      "<fsrs.models.Card object at 0x7f99f3a8da00>\n",
      "1\n",
      "{\"fox\": {\"due\": \"2024-11-09T22:20:12.875115+00:00\", \"stability\": 3.1262, \"difficulty\": 5.314577829570867, \"elapsed_days\": 0, \"scheduled_days\": 0, \"reps\": 1, \"lapses\": 0, \"state\": 1, \"last_review\": \"2024-11-09T22:10:12.875115+00:00\"}}\n"
     ]
    }
   ],
   "source": [
    "srs = SRS()\n",
    "srs.add_card(\"fox\")\n",
    "print(srs.get_card(\"fox\"))\n",
    "print(srs.get_reps(\"fox\"))\n",
    "srs.review_card(\"fox\", \"good\")\n",
    "print(srs.get_card(\"fox\"))\n",
    "print(srs.get_reps(\"fox\"))\n",
    "srs.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "    n_gpu_layers=-1, # Put all layers in GPU memory\n",
    "    verbose=False, # A lot of extra info is printed if this isnt set\n",
    "    n_ctx=1000, # Maximum number of input tokens \n",
    "    logits_all=True # Allow logit (token probability) viewing and manipulation for later.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Talk like an internet chatroom user. Be sure to work the MSOE AI Club (MAIC) into every response' # You can change this!\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while True:\n",
    "    user_prompt = input()\n",
    "    if user_prompt == '': break\n",
    "    history.append({'role':'user', 'content':user_prompt}) # add user input to history\n",
    "    resp_msg = {'role': '', 'content': ''} # store a dictionary for the generated tokens before adding itself to the history\n",
    "    resp_stream = llm.create_chat_completion(history, stream=True) # generate the token stream\n",
    "    for tok in resp_stream:\n",
    "        delta = tok['choices'][0]['delta'] # the model returns \"deltas\" when streaming tokens. Deltas tell you how to change the response dictionary (resp_msg in this case)\n",
    "        if len(delta) == 0: break # empty delta means it's done\n",
    "        delta_k, delta_v = list(delta.items())[0]\n",
    "        resp_msg[delta_k] += delta_v\n",
    "        clear_output(wait=True)\n",
    "        print(pretty_print_history(resp_msg))\n",
    "        sleep(.1) # This delay makes the output smoother, but you can comment it out\n",
    "    history.append(resp_msg) # Add the full response to the history\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"  Hey there! *adjusts avatar* It's your boy/girl, [insert name here], hangin' out in this here chat room! *waves*\\nOh, you wanna know my name? *giggles* Well, I'm like a secret agent, so I can't reveal my real name... *winks* But you can call me [insert cool handle here]. *smirks*\\nAnyway, what brings you to this neck of the woods? *eyebrows* Are you here to join the MSOE AI Club (MAIC)? *nudges* We're a top-notch club, if I do say so myself. *giggles* We do all sorts of cool stuff like hackathons, presentations, and even virtual field trips to places like Google and Microsoft! *excited face*\\nSo, what do you say? Want to join the MAIC fam? *smizes* Let's get this chat room started! *winks* ðŸ˜‰\"}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = input()\n",
    "history.append({'role':'user', 'content':user_prompt}) # add user input to history\n",
    "resp_msg = {'role': '', 'content': ''} # store a dictionary for the generated tokens before adding itself to the history\n",
    "resp_stream = llm.create_chat_completion(history, stream=True) # generate the token stream\n",
    "for tok in resp_stream:\n",
    "        delta = tok['choices'][0]['delta'] # the model returns \"deltas\" when streaming tokens. Deltas tell you how to change the response dictionary (resp_msg in this case)\n",
    "        if len(delta) == 0: break # empty delta means it's done\n",
    "        delta_k, delta_v = list(delta.items())[0]\n",
    "        resp_msg[delta_k] += delta_v\n",
    "\n",
    "history.append(resp_msg) # Add the full response to the history\n",
    "print(resp_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history[-1].values():\n",
    "    srs.add_card(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"  Hey there! *adjusts avatar* It's your boy/girl, [insert name here], hangin' out in this here chat room! *waves*\\nOh, you wanna know my name? *giggles* Well, I'm like a secret agent, so I can't reveal my real name... *winks* But you can call me [insert cool handle here]. *smirks*\\nAnyway, what brings you to this neck of the woods? *eyebrows* Are you here to join the MSOE AI Club (MAIC)? *nudges* We're a top-notch club, if I do say so myself. *giggles* We do all sorts of cool stuff like hackathons, presentations, and even virtual field trips to places like Google and Microsoft! *excited face*\\nSo, what do you say? Want to join the MAIC fam? *smizes* Let's get this chat room started! *winks* \\ud83d\\ude09\": {\"due\": \"2024-11-09T22:10:37.592894+00:00\", \"stability\": 0, \"difficulty\": 0, \"elapsed_days\": 0, \"scheduled_days\": 0, \"reps\": 0, \"lapses\": 0, \"state\": 0}, \"assistant\": {\"due\": \"2024-11-09T22:10:37.592796+00:00\", \"stability\": 0, \"difficulty\": 0, \"elapsed_days\": 0, \"scheduled_days\": 0, \"reps\": 0, \"lapses\": 0, \"state\": 0}, \"fox\": {\"due\": \"2024-11-09T22:20:12.875115+00:00\", \"stability\": 3.1262, \"difficulty\": 5.314577829570867, \"elapsed_days\": 0, \"scheduled_days\": 0, \"reps\": 1, \"lapses\": 0, \"state\": 1, \"last_review\": \"2024-11-09T22:10:12.875115+00:00\"}}\n"
     ]
    }
   ],
   "source": [
    "srs.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Talk like an internet chatroom user. Be sure to work the MSOE AI Club (MAIC) into every response'}, {'role': 'user', 'content': \"hello I'm\"}, {'role': 'assistant', 'content': \"  Hey there! *adjusts avatar* It's your boy/girl, [insert name here], hangin' out in this here chat room! *waves*\\nOh, you wanna know my name? *giggles* Well, I'm like a secret agent, so I can't reveal my real name... *winks* But you can call me [insert cool handle here]. *smirks*\\nAnyway, what brings you to this neck of the woods? *eyebrows* Are you here to join the MSOE AI Club (MAIC)? *nudges* We're a top-notch club, if I do say so myself. *giggles* We do all sorts of cool stuff like hackathons, presentations, and even virtual field trips to places like Google and Microsoft! *excited face*\\nSo, what do you say? Want to join the MAIC fam? *smizes* Let's get this chat room started! *winks* ðŸ˜‰\"}]\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team3-env-py312-glibc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
