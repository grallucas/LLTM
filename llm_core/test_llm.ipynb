{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_core.llm as L\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'constants': [{'name': 'pi', 'value': 3.141592653589793}, {'name': 'e', 'value': 2.718281828459045}]}\n"
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! Give me a few mathematial constants?',\n",
    "    response_format={\n",
    "        'constants': [{'name': str, 'value': float}, ...]\n",
    "    },\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! What are some Wisconsin cities?',\n",
    "    response_format={'country': str, 'cities': [{'city_name': str, 'population': int}, 10]},\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['cities'][2]['city_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.LLM.detokenize(llm._hist_to_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class Repr:\n",
    "    def __init__(self, s): self.s = s\n",
    "    def __str__(self): return self.s\n",
    "    def __repr__(self): return self.s\n",
    "\n",
    "def pretty_response_format(response_format):\n",
    "    if response_format in [str, int, float]:\n",
    "        return Repr(response_format.__name__)\n",
    "    elif type(response_format) == dict:\n",
    "        return {\n",
    "            Repr(f'\"{k}\"'): pretty_response_format(v)\n",
    "            for k,v in response_format.items()\n",
    "        }\n",
    "    elif type(response_format) == list:\n",
    "        sub, etc = response_format\n",
    "\n",
    "        if etc == ...:\n",
    "            return [pretty_response_format(sub), Repr('...')]\n",
    "        elif type(etc) is int:\n",
    "            return [pretty_response_format(sub), Repr(f'{etc} total...')]\n",
    "\n",
    "        else:\n",
    "            raise Exception(f'List size must be int or `...`, but found: {etc}')\n",
    "\n",
    "    else:\n",
    "        raise Exception(f'Unsupported value type: {v}')\n",
    "\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'local_names': [str, ...]}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_country': {'name': str, 'pop': int}}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_cities': [{'name': str, 'pop': int}, 3]}\n",
    "fmt = str\n",
    "\n",
    "fmt_prompt = pretty_response_format(fmt)\n",
    "\n",
    "# print(str(fmt_prompt))\n",
    "# print(f'{fmt_prompt}')\n",
    "# assert 0\n",
    "\n",
    "def gen_response_formated_rec(response_format, prompt_ref, temperature=0, max_tokens=1000):\n",
    "    if response_format == str:\n",
    "        prompt_ref[0] += '\"'\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=str_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span(2)[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "        prompt_ref[0] += '\"'\n",
    "\n",
    "    elif response_format in [float, int]:\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=num_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'[,}]', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span()[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "\n",
    "    elif type(response_format) == dict:\n",
    "        prompt_ref[0] += '{'\n",
    "        first = True\n",
    "        for k,v in response_format.items():\n",
    "            if not first: prompt_ref[0] += ', '\n",
    "            first = False\n",
    "            prompt_ref[0] += f'\"{k}\": '\n",
    "            gen_response_formated_rec(v, prompt_ref, temperature, max_tokens)\n",
    "        prompt_ref[0] += '}'\n",
    "\n",
    "    elif type(response_format) == list:\n",
    "        sub_type, etc = response_format\n",
    "\n",
    "        i = None\n",
    "        if type(etc) is int:\n",
    "            i = etc\n",
    "        else:\n",
    "            assert etc == ...\n",
    "\n",
    "        prompt_ref[0] += '['\n",
    "        while True:\n",
    "            if not i is None:\n",
    "                if i == 0: break\n",
    "                i -= 1\n",
    "\n",
    "            gen_response_formated_rec(sub_type, prompt_ref, temperature, max_tokens)\n",
    "            next_tok = L.LLM_GLOBAL_INSTANCE(\n",
    "                prompt_ref[0],\n",
    "                max_tokens=1, temperature=0,\n",
    "                stream=False,\n",
    "            )['choices'][0]['text'].strip()\n",
    "            \n",
    "            if ']' in next_tok:\n",
    "                break\n",
    "            elif ',' in next_tok:\n",
    "                prompt_ref[0] += ', '\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f'Theoretically unreachable err: invalid list next tok: {next_tok}')\n",
    "\n",
    "        prompt_ref[0] += ']'\n",
    "\n",
    "def gen_response_formated(response_format, prompt, temperature=0, max_tokens=1000):\n",
    "    json_start = len(prompt)\n",
    "    prompt_ref = [prompt]\n",
    "    gen_response_formated_rec(fmt, prompt_ref, temperature, max_tokens)\n",
    "    return prompt_ref[0][json_start:]\n",
    "\n",
    "prompt = f'[INST] What is one capital city in Europe? Respond in JSON with this format ending strings with double quotes and numbers with JS notation (commas end rather than separate digits): {fmt_prompt} [/INST] '\n",
    "print(prompt)\n",
    "\n",
    "resp = gen_response_formated(fmt, prompt)\n",
    "\n",
    "print(resp)\n",
    "\n",
    "# assert 0\n",
    "\n",
    "# json_start = len(prompt)\n",
    "# prompt += '{'\n",
    "\n",
    "\n",
    "# first = True\n",
    "# for k,v in fmt.items():\n",
    "#     if not first: prompt += ', '\n",
    "#     first = False\n",
    "#     prompt += f'\"{k}\": '\n",
    "#     if v == str:\n",
    "#         prompt += '\"'\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=str_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span(2)[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#         prompt += '\"'\n",
    "#     elif v in [int, float]:\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=num_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'[,}]', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span()[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#     else:\n",
    "#         raise Exception(f\"Unsupported value type: {v}\")\n",
    "# prompt += '}'\n",
    "\n",
    "# resp = json.loads(prompt[json_start:])\n",
    "\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    json.loads('owegboieqob')\n",
    "except json.JSONDecodeError as e:\n",
    "    raise Exception(f'Cannot parse this json: 23223542')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a Finnish language teacher teaching Finnish to me through conversation. You must give accurate information.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Oletko koira? respond in English.', response_format='stream', max_tokens=1000, temperature=0.1)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Why did you start your response with an extra space????? AHHHH!!', response_format='stream', max_tokens=1000, temperature=0)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm._hist_to_prompt()\n",
    "L.LLM.detokenize(llm._hist_to_prompt())\n",
    "# print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = L.LLM_GLOBAL_INSTANCE.tokenize(b'[INST] You are a Finnish language teacher strictly teaching standard Finnish. You must give accurate information. [/INST] Understood.', add_bos=False)\n",
    "ts = [1, *ts, 2]\n",
    "ts += L.LLM_GLOBAL_INSTANCE.tokenize(' [INST] What does \"mitä on sinun nimi\" mean? 1) What is another way to say it? 2) Why is the original version NOT grammatically correct? Tell me in terms of every individual word. [/INST] '.encode('utf-8'), add_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L.LLM.detokenize(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = L.LLM_GLOBAL_INSTANCE(ts, temperature=0, max_tokens=1000, stream=True)\n",
    "for tok in resp:\n",
    "    print(tok['choices'][0]['text'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Hello. How are yo?', response_format={'my_name':'...', 'my_response':'...'}, max_tokens=1000)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$$  TODO ^ remove the \"respond in json\" instruction in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_processor(prev_tok_ids, next_tok_logits):\n",
    "    \n",
    "    # print(prev_tok_ids, next_tok_logits)\n",
    "\n",
    "    next_tok_logits[995] -= 1000\n",
    " \n",
    "    return next_tok_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = L.LLM('You are a British butler. Start every answer with \"you\"')\n",
    "llm = L.LLM('You are a British assistant.')\n",
    "resp = llm('Hello. What is the capital of Finland?', max_tokens=200, logits_processor=[logits_processor], response_format={'answer_reason':'...', 'answer':'...'})\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.get_pretty_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.tokenize(' You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.detokenize([1, 995])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
