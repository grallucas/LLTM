{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_core.llm as L\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'constants': [{'name': 'pi', 'value': 3.141592653589793}, {'name': 'e', 'value': 2.718281828459045}]}\n"
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! Give me a few mathematial constants?',\n",
    "    response_format={\n",
    "        'constants': [{'name': str, 'value': float}, ...]\n",
    "    },\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! What are some Wisconsin cities?',\n",
    "    response_format={'country': str, 'cities': [{'city_name': str, 'population': int}, 10]},\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['cities'][2]['city_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.LLM.detokenize(llm._hist_to_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class Repr:\n",
    "    def __init__(self, s): self.s = s\n",
    "    def __str__(self): return self.s\n",
    "    def __repr__(self): return self.s\n",
    "\n",
    "def pretty_response_format(response_format):\n",
    "    if response_format in [str, int, float]:\n",
    "        return Repr(response_format.__name__)\n",
    "    elif type(response_format) == dict:\n",
    "        return {\n",
    "            Repr(f'\"{k}\"'): pretty_response_format(v)\n",
    "            for k,v in response_format.items()\n",
    "        }\n",
    "    elif type(response_format) == list:\n",
    "        sub, etc = response_format\n",
    "\n",
    "        if etc == ...:\n",
    "            return [pretty_response_format(sub), Repr('...')]\n",
    "        elif type(etc) is int:\n",
    "            return [pretty_response_format(sub), Repr(f'{etc} total...')]\n",
    "\n",
    "        else:\n",
    "            raise Exception(f'List size must be int or `...`, but found: {etc}')\n",
    "\n",
    "    else:\n",
    "        raise Exception(f'Unsupported value type: {v}')\n",
    "\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'local_names': [str, ...]}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_country': {'name': str, 'pop': int}}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_cities': [{'name': str, 'pop': int}, 3]}\n",
    "fmt = str\n",
    "\n",
    "fmt_prompt = pretty_response_format(fmt)\n",
    "\n",
    "# print(str(fmt_prompt))\n",
    "# print(f'{fmt_prompt}')\n",
    "# assert 0\n",
    "\n",
    "def gen_response_formated_rec(response_format, prompt_ref, temperature=0, max_tokens=1000):\n",
    "    if response_format == str:\n",
    "        prompt_ref[0] += '\"'\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=str_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span(2)[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "        prompt_ref[0] += '\"'\n",
    "\n",
    "    elif response_format in [float, int]:\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=num_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'[,}]', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span()[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "\n",
    "    elif type(response_format) == dict:\n",
    "        prompt_ref[0] += '{'\n",
    "        first = True\n",
    "        for k,v in response_format.items():\n",
    "            if not first: prompt_ref[0] += ', '\n",
    "            first = False\n",
    "            prompt_ref[0] += f'\"{k}\": '\n",
    "            gen_response_formated_rec(v, prompt_ref, temperature, max_tokens)\n",
    "        prompt_ref[0] += '}'\n",
    "\n",
    "    elif type(response_format) == list:\n",
    "        sub_type, etc = response_format\n",
    "\n",
    "        i = None\n",
    "        if type(etc) is int:\n",
    "            i = etc\n",
    "        else:\n",
    "            assert etc == ...\n",
    "\n",
    "        prompt_ref[0] += '['\n",
    "        while True:\n",
    "            if not i is None:\n",
    "                if i == 0: break\n",
    "                i -= 1\n",
    "\n",
    "            gen_response_formated_rec(sub_type, prompt_ref, temperature, max_tokens)\n",
    "            next_tok = L.LLM_GLOBAL_INSTANCE(\n",
    "                prompt_ref[0],\n",
    "                max_tokens=1, temperature=0,\n",
    "                stream=False,\n",
    "            )['choices'][0]['text'].strip()\n",
    "            \n",
    "            if ']' in next_tok:\n",
    "                break\n",
    "            elif ',' in next_tok:\n",
    "                prompt_ref[0] += ', '\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f'Theoretically unreachable err: invalid list next tok: {next_tok}')\n",
    "\n",
    "        prompt_ref[0] += ']'\n",
    "\n",
    "def gen_response_formated(response_format, prompt, temperature=0, max_tokens=1000):\n",
    "    json_start = len(prompt)\n",
    "    prompt_ref = [prompt]\n",
    "    gen_response_formated_rec(fmt, prompt_ref, temperature, max_tokens)\n",
    "    return prompt_ref[0][json_start:]\n",
    "\n",
    "prompt = f'[INST] What is one capital city in Europe? Respond in JSON with this format ending strings with double quotes and numbers with JS notation (commas end rather than separate digits): {fmt_prompt} [/INST] '\n",
    "print(prompt)\n",
    "\n",
    "resp = gen_response_formated(fmt, prompt)\n",
    "\n",
    "print(resp)\n",
    "\n",
    "# assert 0\n",
    "\n",
    "# json_start = len(prompt)\n",
    "# prompt += '{'\n",
    "\n",
    "\n",
    "# first = True\n",
    "# for k,v in fmt.items():\n",
    "#     if not first: prompt += ', '\n",
    "#     first = False\n",
    "#     prompt += f'\"{k}\": '\n",
    "#     if v == str:\n",
    "#         prompt += '\"'\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=str_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span(2)[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#         prompt += '\"'\n",
    "#     elif v in [int, float]:\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=num_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'[,}]', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span()[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#     else:\n",
    "#         raise Exception(f\"Unsupported value type: {v}\")\n",
    "# prompt += '}'\n",
    "\n",
    "# resp = json.loads(prompt[json_start:])\n",
    "\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    json.loads('owegboieqob')\n",
    "except json.JSONDecodeError as e:\n",
    "    raise Exception(f'Cannot parse this json: 23223542')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a Finnish language teacher teaching Finnish to me through conversation. You must give accurate information.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Oletko koira? respond in English.', response_format='stream', max_tokens=1000, temperature=0.1)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Why did you start your response with an extra space????? AHHHH!!', response_format='stream', max_tokens=1000, temperature=0)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm._hist_to_prompt()\n",
    "L.LLM.detokenize(llm._hist_to_prompt())\n",
    "# print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = L.LLM_GLOBAL_INSTANCE.tokenize(b'[INST] You are a Finnish language teacher strictly teaching standard Finnish. You must give accurate information. [/INST] Understood.', add_bos=False)\n",
    "ts = [1, *ts, 2]\n",
    "ts += L.LLM_GLOBAL_INSTANCE.tokenize(' [INST] What does \"mit√§ on sinun nimi\" mean? 1) What is another way to say it? 2) Why is the original version NOT grammatically correct? Tell me in terms of every individual word. [/INST] '.encode('utf-8'), add_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L.LLM.detokenize(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = L.LLM_GLOBAL_INSTANCE(ts, temperature=0, max_tokens=1000, stream=True)\n",
    "for tok in resp:\n",
    "    print(tok['choices'][0]['text'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Hello. How are yo?', response_format={'my_name':'...', 'my_response':'...'}, max_tokens=1000)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$$  TODO ^ remove the \"respond in json\" instruction in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_processor(prev_tok_ids, next_tok_logits):\n",
    "    \n",
    "    # print(prev_tok_ids, next_tok_logits)\n",
    "\n",
    "    next_tok_logits[995] -= 1000\n",
    " \n",
    "    return next_tok_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = L.LLM('You are a British butler. Start every answer with \"you\"')\n",
    "llm = L.LLM('You are a British assistant.')\n",
    "resp = llm('Hello. What is the capital of Finland?', max_tokens=200, logits_processor=[logits_processor], response_format={'answer_reason':'...', 'answer':'...'})\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.get_pretty_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.tokenize(' You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.detokenize([1, 995])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
