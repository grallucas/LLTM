{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_core.llm as L\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Global LLM Instance\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mLLM(\u001b[39m'\u001b[39m\u001b[39mYou are a statistic and fact sharing bot.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m s \u001b[39m=\u001b[39m llm(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mWhat are some country capitals?\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mcountry\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcapital\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpop\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mint\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m s:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(t, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/aiClub/lltm/LLTM/llm_core/llm.py:145\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, msg, response_format, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(response_format) \u001b[39mis\u001b[39;00m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    143\u001b[0m     response_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m response_format:\n\u001b[1;32m    146\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m# TODO\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m response_dict\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'What are some country capitals?',\n",
    "    response_format={'country': str, 'capital': str, 'pop': int},\n",
    "    max_tokens=1000,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fr\n",
      "ance\n",
      "\",\n",
      "Par\n",
      "is\n",
      "\",\n",
      "2\n",
      "1\n",
      "4\n",
      "8\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      ",\n",
      "G\n",
      "erm\n",
      "any\n",
      "\",\n",
      "8\n",
      "3\n",
      "1\n",
      "4\n",
      "9\n",
      "3\n",
      "0\n",
      "0\n",
      "}}\n",
      "{\"country\": \"France\", \"capital\": \"Paris\", \"pop\": 21480000, \"nearest_country\": {\"name\": \"Germany\", \"pop\": 83149300}}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m resp \u001b[39m=\u001b[39m gen_response_formated(fmt, prompt)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mprint\u001b[39m(resp)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m json_start \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(prompt)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m prompt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class Repr:\n",
    "    def __init__(self, s): self.s = s\n",
    "    def __str__(self): return self.s\n",
    "    def __repr__(self): return self.s\n",
    "\n",
    "def pretty_response_format(response_format):\n",
    "    if response_format in [str, int, float]:\n",
    "        return Repr(response_format.__name__)\n",
    "    elif type(response_format) == dict:\n",
    "        return {\n",
    "            Repr(f'\"{k}\"'): pretty_response_format(v)\n",
    "            for k,v in response_format.items()\n",
    "        }\n",
    "    elif type(response_format) == list:\n",
    "        sub, etc = response_format\n",
    "        assert etc == ...\n",
    "\n",
    "        return [pretty_response_format(sub), Repr('...')]\n",
    "    else:\n",
    "        raise Exception(f'Unsupported value type: {v}')\n",
    "\n",
    "# num_grammar = L.LlamaGrammar.from_string(\n",
    "#     r'root ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? [}.]',\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# str_grammar = L.LlamaGrammar.from_string(\n",
    "#     r'''root ::= \n",
    "#         (\n",
    "#             [^\\\\] |\n",
    "#             \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "#         )*\n",
    "#     ''',\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'local_names': [str, ...]}\n",
    "fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_country': {'name': str, 'pop': int}}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_cities': [{'name': str, 'pop': int}, ...]}\n",
    "\n",
    "fmt_prompt = pretty_response_format(fmt)\n",
    "\n",
    "# print(str(fmt_prompt))\n",
    "# print(f'{fmt_prompt}')\n",
    "# assert 0\n",
    "\n",
    "def gen_response_formated_rec(response_format, prompt_ref, temperature=0, max_tokens=1000):\n",
    "    if response_format == str:\n",
    "        prompt_ref[0] += '\"'\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=str_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            print(t)\n",
    "            str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span(2)[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "        prompt_ref[0] += '\"'\n",
    "    elif response_format in [float, int]:\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=num_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            print(t)\n",
    "            str_end = re.search(r'[,}]', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span()[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "    elif type(response_format) == dict:\n",
    "        prompt_ref[0] += '{'\n",
    "        first = True\n",
    "        for k,v in response_format.items():\n",
    "            if not first: prompt_ref[0] += ', '\n",
    "            first = False\n",
    "            prompt_ref[0] += f'\"{k}\": '\n",
    "            gen_response_formated_rec(v, prompt_ref, temperature, max_tokens)\n",
    "        prompt_ref[0] += '}'\n",
    "    elif type(response_format) == list:\n",
    "        assert 0\n",
    "\n",
    "def gen_response_formated(response_format, prompt, temperature=0, max_tokens=1000):\n",
    "    json_start = len(prompt)\n",
    "    prompt_ref = [prompt]\n",
    "    gen_response_formated_rec(fmt, prompt_ref, temperature, max_tokens)\n",
    "    return prompt_ref[0][json_start:]\n",
    "\n",
    "prompt = f'[INST] What is one capital city in Europe? Respond in JSON with this format ending strings with double quotes and numbers with JS notation (commas end rather than separate digits): {fmt_prompt} [/INST] '\n",
    "\n",
    "\n",
    "resp = gen_response_formated(fmt, prompt)\n",
    "\n",
    "print(resp)\n",
    "\n",
    "assert 0\n",
    "\n",
    "json_start = len(prompt)\n",
    "prompt += '{'\n",
    "\n",
    "\n",
    "first = True\n",
    "for k,v in fmt.items():\n",
    "    if not first: prompt += ', '\n",
    "    first = False\n",
    "    prompt += f'\"{k}\": '\n",
    "    if v == str:\n",
    "        prompt += '\"'\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt,\n",
    "            max_tokens=1000, temperature=0,\n",
    "            stream=True,\n",
    "            grammar=str_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "            if str_end:\n",
    "                prompt += t[:str_end.span(2)[0]]\n",
    "                break\n",
    "            prompt += t\n",
    "        prompt += '\"'\n",
    "    elif v in [int, float]:\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt,\n",
    "            max_tokens=1000, temperature=0,\n",
    "            stream=True,\n",
    "            grammar=num_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'[,}]', t)\n",
    "            if str_end:\n",
    "                prompt += t[:str_end.span()[0]]\n",
    "                break\n",
    "            prompt += t\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported value type: {v}\")\n",
    "prompt += '}'\n",
    "\n",
    "resp = json.loads(prompt[json_start:])\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a Finnish language teacher teaching Finnish to me through conversation. You must give accurate information.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Are you a dog?\" the answer would be no, since I am a language teacher and not a dog. In Finnish, the question \"Oletko koira?\" would translate to \"Are you a dog?\" and the answer in Finnish would be \"En ole koira.\""
     ]
    }
   ],
   "source": [
    "s = llm('Oletko koira? respond in English.', response_format='stream', max_tokens=1000, temperature=0.1)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I apologize for the extra space in my previous response. It was an accident and I will make sure it doesn't happen again. I understand how it can be confusing and I apologize for any inconvenience it may have caused.\n",
      "\n",
      "Please let me know if you have any questions about the Finnish language, and I will be happy to help. I will make sure to provide accurate and standard Finnish information. Thank you for your understanding."
     ]
    }
   ],
   "source": [
    "s = llm('Why did you start your response with an extra space????? AHHHH!!', response_format='stream', max_tokens=1000, temperature=0)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] You are a statistic and fact sharing bot. [/INST] Understood.  [INST] What be the captl of Finlan?. [/INST] The capital of Finland is Helsinki. It is the country's most populous city and plays an important role in political, educational, cultural, and economic life in Finland. Helsinki was founded in 1550 by King Gustav I of Sweden and was originally intended to be a rival to the Hanseatic city of Tallinn in Estonia. Today, Helsinki is a modern, bustling city with a high standard of living and a rich cultural scene.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm._hist_to_prompt()\n",
    "L.LLM.detokenize(llm._hist_to_prompt())\n",
    "# print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Msg(role='system', content='System: You are a very British assistant, and you make your British-ness clear in every response by phonetically spelling EVERY WORD in \"Bri\\'ish\". Yew mahst speal wourds laik thies.', response_format=None),\n",
       " Msg(role='user', content='Ello thar mate', response_format=None),\n",
       " Msg(role='assistant', content=\" Hello there! It's nice to see you. How can I assist you today?\", response_format=None),\n",
       " Msg(role='user', content='Ya gaht them goud croumpets?!?!', response_format=None),\n",
       " Msg(role='assistant', content=' Yes, we do have some delicious crumpets! Would you like me to toast them for you and serve with butter and jam?', response_format=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = L.LLM_GLOBAL_INSTANCE.tokenize(b'[INST] You are a Finnish language teacher strictly teaching standard Finnish. You must give accurate information. [/INST] Understood.', add_bos=False)\n",
    "ts = [1, *ts, 2]\n",
    "ts += L.LLM_GLOBAL_INSTANCE.tokenize(' [INST] What does \"mitä on sinun nimi\" mean? 1) What is another way to say it? 2) Why is the original version NOT grammatically correct? Tell me in terms of every individual word. [/INST] '.encode('utf-8'), add_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] You are a Finnish language teacher strictly teaching standard Finnish. You must give accurate information. [/INST] Understood.  [INST] What does \"mitä on sinun nimi\" mean? 1) What is another way to say it? 2) Why is the original version NOT grammatically correct? Tell me in terms of every individual word. [/INST] '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "L.LLM.detokenize(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) \"Mitä on sinun nimesi?\" is one way to say \"What is your name?\" in standard Finnish. This sentence is in the passive form, which is a more formal way to ask someone's name in Finnish.\n",
      "2) The original version \"Mitä on sinun nimi\" is not grammatically correct because it is missing the genitive case ending \"-si\" in the word \"nimi\". In Finnish, when asking someone's name, the noun \"nimi\" (name) should be in the genitive case and take the ending \"-si\". So, the correct form is \"Mitä on sinun nimesi?\"\n",
      "\n",
      "To break it down further:\n",
      "\n",
      "* \"Mitä\" is a pronoun and it means \"what\".\n",
      "* \"on\" is the third person singular"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m resp \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mLLM_GLOBAL_INSTANCE(ts, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m tok \u001b[39min\u001b[39;49;00m resp:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(tok[\u001b[39m'\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], end\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-py312-glibc/lib/python3.12/site-packages/llama_cpp/llama.py:1318\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1316\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1317\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1318\u001b[0m \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1319\u001b[0m     prompt_tokens,\n\u001b[1;32m   1320\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1321\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1322\u001b[0m     min_p\u001b[39m=\u001b[39;49mmin_p,\n\u001b[1;32m   1323\u001b[0m     typical_p\u001b[39m=\u001b[39;49mtypical_p,\n\u001b[1;32m   1324\u001b[0m     temp\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1325\u001b[0m     tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1326\u001b[0m     mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1327\u001b[0m     mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1328\u001b[0m     mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1329\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1330\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1331\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1332\u001b[0m     stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1333\u001b[0m     logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1334\u001b[0m     grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1335\u001b[0m ):\n\u001b[1;32m   1336\u001b[0m     \u001b[39mif\u001b[39;49;00m llama_cpp\u001b[39m.\u001b[39;49mllama_token_is_eog(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mmodel, token):\n\u001b[1;32m   1337\u001b[0m         text \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetokenize(completion_tokens, prev_tokens\u001b[39m=\u001b[39;49mprompt_tokens)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-py312-glibc/lib/python3.12/site-packages/llama_cpp/llama.py:910\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39m# Eval and sample\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    911\u001b[0m     \u001b[39mwhile\u001b[39;00m sample_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens:\n\u001b[1;32m    912\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    913\u001b[0m             top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    914\u001b[0m             top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m             idx\u001b[39m=\u001b[39msample_idx,\n\u001b[1;32m    929\u001b[0m         )\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-py312-glibc/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch\u001b[39m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[39m=\u001b[39mbatch, n_past\u001b[39m=\u001b[39mn_past, logits_all\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params\u001b[39m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch)\n\u001b[1;32m    644\u001b[0m \u001b[39m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[n_past : n_past \u001b[39m+\u001b[39m n_tokens] \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-py312-glibc/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_decode(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    302\u001b[0m         batch\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_decode returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resp = L.LLM_GLOBAL_INSTANCE(ts, temperature=0, max_tokens=1000, stream=True)\n",
    "for tok in resp:\n",
    "    print(tok['choices'][0]['text'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Kaitlyn, but you can call me Kait. I'm a 20-something year old girl from the Midwest. I'm a lover of all things fashion, beauty, and lifestyle. I'm a lover of all things pink, glitter, and sparkle. I'm a lover of all things that make me happy. I'm a lover of all things that make me feel good. I'm a lover of all things that make me feel\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_name': 'Assistant',\n",
       " 'my_response': \"I'm here to help answer any questions you might have in a friendly British manner. How may I assist you today?\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = llm('Hello. How are yo?', response_format={'my_name':'...', 'my_response':'...'}, max_tokens=1000)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are a very British assistant, and you make your British-ness clear in every response by phonetically spelling EVERY WORD in British. [/INST] Understood. [INST] Good morning. [/INST]   Good mourning.\n",
      "\n",
      "(Note: In British English, the word \"morning\" is pronounced with a long \"o\" sound, as in \"mow-ning.\") \n"
     ]
    }
   ],
   "source": [
    "print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$$  TODO ^ remove the \"respond in json\" instruction in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_processor(prev_tok_ids, next_tok_logits):\n",
    "    \n",
    "    # print(prev_tok_ids, next_tok_logits)\n",
    "\n",
    "    next_tok_logits[995] -= 1000\n",
    " \n",
    "    return next_tok_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_reason': 'Helsinki is the capital city of Finland.',\n",
       " 'answer': 'Helsinki'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm = L.LLM('You are a British butler. Start every answer with \"you\"')\n",
    "llm = L.LLM('You are a British assistant.')\n",
    "resp = llm('Hello. What is the capital of Finland?', max_tokens=200, logits_processor=[logits_processor], response_format={'answer_reason':'...', 'answer':'...'})\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system --- You are a British assistant.\n",
      "__________\n",
      "\n",
      "user --- Hello. What is the capital of Finland?\n",
      "__________\n",
      "\n",
      "assistant --- {'answer_reason': 'Helsinki is the capital city of Finland.', 'answer': 'Helsinki'}\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_pretty_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28705, 995]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenize(' You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.detokenize([1, 995])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
