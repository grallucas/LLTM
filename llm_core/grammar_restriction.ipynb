{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_core.llm as L\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce available words to see difference in grammar\n",
    "# n-5 vocab\n",
    "\n",
    "# better benchmarking rules\n",
    "# LLM as judge to an extent (multiple?)\n",
    "# grade out of 100 with rubric\n",
    "# do by hand too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['MODEL_PATH'] = \"/data/ai_club/llms/qwen2-7b-instruct-q5_k_m.gguf\"\n",
    "LLM_GLOBAL_INSTANCE = L.Llama(\n",
    "                n_ctx=1000,\n",
    "                model_path=os.environ['MODEL_PATH'],\n",
    "                n_gpu_layers=-1, verbose=0,\n",
    "                # embedding=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_input = \"You are a Japanese language assistant who speaks in consice, simple sentences.\"\n",
    "# user_input = \"Tell me about your favorite Kanji, in Japanese using Kanji, Hiragana, and Katakana when appropriate\"\n",
    "system_input = \"You are a helpful assistant. Respond with prompts suitable for Japanese children's books, following the [word] が [word] です form.\"\n",
    "user_input = \"赤色は何をしてるですか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-a0d28538-0d15-4bbc-babe-8b2bc0067abc',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1740143766,\n",
       " 'model': '/data/ai_club/llms/qwen2.5-7b-instruct-q8_0-00001-of-00003.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': 'あがめです。'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 50, 'completion_tokens': 5, 'total_tokens': 55}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No Kanji usage, rare katakana usage\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence{1,2}\n",
    "allowedchars ::= [\\u3040-\\u30FF, \\u4e00 - \\u9faf]\n",
    "word ::= allowedchars\n",
    "sentence ::= word particle word desu end\n",
    "particle ::= \"が\"\n",
    "desu ::= \"です\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)\n",
    "\n",
    "LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ], \n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-77bc66c8-bc91-4339-b8e4-fc5140beaa4a',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1740143768,\n",
       " 'model': '/data/ai_club/llms/qwen2.5-7b-instruct-q8_0-00001-of-00003.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': '赤色は炎のように光ってます が 楽しく感じます です。'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 50, 'completion_tokens': 19, 'total_tokens': 69}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With basic prompt, no character restrictions spamps \\u0000 or \\uffff\n",
    "#Good responses with user_input = \"Tell me about your favorite Kanji, in Japanese using Kanji, Hiragana, and Katakana when appropriate\"\n",
    "#Talks in any vocab, but keeps sentences in が + です format. :D\n",
    "# Try telling it to write with spaces between words?\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence{1,2}\n",
    "allowedchars ::= [\\u4e00 - \\u9faf, \\u3000-\\u303f, \\u3040-\\u309F, \\u30a0-\\u30ff]\n",
    "word ::= (unicodechar)+\n",
    "unicodechar ::= [\\u0000-\\uffff]\n",
    "sentence ::= word particle word desu end\n",
    "particle ::= \"が\"\n",
    "desu ::= \"です\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)\n",
    "\n",
    "LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ], \n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )\n",
    "\n",
    "#Really good example sentence: '赤色は炎のように光ってます が 楽しく感じます です。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_GLOBAL_INSTANCE.reset()\n",
    "system_input = \"You are a helpful assistant. Respond with prompts suitable for Japanese children's books, following the [word] が [word] です form.\"\n",
    "user_input = \"赤色は何をしてるですか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 1 or 2 allowedchar makes for much shorter+concise sentences\n",
    "#adding more sentences makes them incoherent\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence\n",
    "allowedchars ::= [\\u0000-\\uffff]\n",
    "word ::= allowedchars\n",
    "sentence ::= word particle word desu end\n",
    "particle ::= \"が\"\n",
    "desu ::= \"です\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)\n",
    "\n",
    "#grammar works better with Japanese prompts\n",
    "#English prompts are better without grammar\n",
    "#Not much variety between responses though\n",
    "#How does Llama grammar work??!!? Logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['赤が喜です。',\n",
       " '赤が喜です。',\n",
       " '赤が火です。',\n",
       " '赤が喜です。',\n",
       " '赤が力です。',\n",
       " '赤が力です。',\n",
       " '赤が怒です。',\n",
       " '赤が喜です。',\n",
       " '赤が怒です。',\n",
       " '赤が光です。']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = list()\n",
    "training_size = 10\n",
    "for i in range(training_size):\n",
    "    response = LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ], \n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )\n",
    "    training_data.append(response['choices'][0][\"message\"][\"content\"])\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i and na-adj don't work well\n",
    "LLM_GLOBAL_INSTANCE.reset()\n",
    "system_input = \"You are a helpful assistant. Respond with prompts suitable for Japanese children's books, using adjectives to describe in the [word] が [adjective] form.\"\n",
    "user_input = \"太陽の景色を描いてください\"\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence\n",
    "allowedchars ::= [\\u0000-\\uffff]\n",
    "word ::= allowedchars\n",
    "sentence ::= word particle adjective shii end\n",
    "particle ::= \"が\"\n",
    "adjective ::= [\\u0000-\\uffff]\n",
    "shii ::= \"しい\"\n",
    "desu ::= \"です\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['太が出しい。',\n",
       " '太が出しい。',\n",
       " '太が出しい。',\n",
       " '太が出しい。',\n",
       " '太が輝しい。',\n",
       " '太があしい。',\n",
       " '太が出しい。',\n",
       " '太があしい。',\n",
       " '太が出しい。',\n",
       " '太が出しい。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = list()\n",
    "training_size = 10\n",
    "for i in range(training_size):\n",
    "    response = LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ], \n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )\n",
    "    training_data.append(response['choices'][0][\"message\"][\"content\"])\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs a good system prompt to use しか at all\n",
    "# Difficult to get it to use しか normally and not just repeat the user question with it.\n",
    "LLM_GLOBAL_INSTANCE.reset()\n",
    "system_input = \"You are a helpful assistant. Respond with prompts using the しか～ない Japanese grammar pattern\"\n",
    "user_input = \"あんたは普通な人って想像してください。。何をできますか？\"\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence\n",
    "allowedchars ::= [\\u0000-\\uffff]\n",
    "word ::= allowedchars+\n",
    "sentence ::= word particle word end\n",
    "particle ::= \"しか\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あなたは普通の人間しか想像できないでしょう。何をできますか？？この質問には、何か特定のスキルや能力があるわけではないので、日常生活の中で必要な基本的な能力が考えられます。例えば、料理を作ったり、掃除を',\n",
       " 'あなたは普通な人って想像してみてください。何をできるか ila しか想像できませんね。',\n",
       " 'あんたは普通な人って想像してください。何か特別なことをするしかないのですね。',\n",
       " 'あんたは普通な人って想像してください。特別なスキルがある場合もあるが、ほとんどは料理するしかありませんな。それも時々しかできませんね。',\n",
       " 'あなたは普通な人って想像してください。何か特別なことをできるしかありませんな。',\n",
       " 'あんたは普通な人って想像してください。何か特別なスキルがあるなんて考えられませんか？もしそんなことがないなら、仕事中はメールチェックするしかありませんね。',\n",
       " '普通な人なら、料理を作ることはほとんどしかありません。何か特別な能力があるのかな？それとも仕事があるのかな？それ以外には何もできない気がするよ。',\n",
       " '普通の人は何か特別なスキルがなく、特殊な状況以外では何もできないという考えでは、少し狭いですね。しかり、基本的な日常生活のスキルしかありません。しかり、何かを始めるためのエネルギーと時間',\n",
       " 'あんたは普通な人って想像してください。そんな人なら料理を作ることはほとんどありませんはずです。しかしこの一つだけはできるかもしれません。しかりあんたなら、お弁当を作るのは難しくないはずです。',\n",
       " 'あんたは普通な人って想像してください。何をできるしかありませんな。']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = list()\n",
    "training_size = 10\n",
    "for i in range(training_size):\n",
    "    response = LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )\n",
    "    training_data.append(response['choices'][0][\"message\"][\"content\"])\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses might be better with grammar? More english junk, but the non-junk are shorter and more straight-forward\n",
    "LLM_GLOBAL_INSTANCE.reset()\n",
    "system_input = \"You are a helpful assistant. Respond with prompts suitable for Japanese children's books, using the かどうか grammar pattern\"\n",
    "user_input = \"Ask me a quesiton in Japanese.\"\n",
    "grammar_string = r\"\"\"\n",
    "root ::= sentence\n",
    "allowedchars ::= [\\u0000-\\uffff]\n",
    "word ::= allowedchars+\n",
    "sentence ::= word particle word end\n",
    "particle ::= \"かどうか\"\n",
    "end ::= \"。\"\n",
    "\"\"\"\n",
    "my_grammar = L.LlamaGrammar.from_string(grammar_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['日本のどの都市に住むかどうか聞かせてください。',\n",
       " '日本語でお聞きBecome a question using the かどうか grammar pattern. \\n\\nあなたは今日お絵かきをするかどうか知りたいです。',\n",
       " \"日本語でお尋ねしますね。この質問には、「かどうか」を使用してみました。\\n\\n絵本の主人公は、新しい友達ができたかどうか知りたいですか？（ Picture book's protagonist wants to know if they made a new friend, かどうか?） \\n\\nこの質問に答えると、主人公の気持ちを理解できるかもしれませんね。\",\n",
       " 'あなたは今日公園に行くかどうか知っていますか？（あなたはきょうこうえんにい행을 가는かどうかしっていますか？） \\n\\nこの文は、今日公園にいくかどうかという質問を子供向けに簡潔に表現しています。\"かどうか\"は、質問の内容を明確にし、どちらの選択肢でも答えがあるということを示しています。',\n",
       " 'あなたは今日学校に行きますか行かないか知りたいです。答え帮我填空吧，可以用 行きます or 行きません。例如：あなたは今日学校に行きます____。填空后句子的意思是“你今天去学校____。”。现在请你填空完成这个句子吧！（あなたは今日学校に行きます____。）填空选项：行きます or 行きません。请根据实际情况填写。例如，如果今天你去学校，就填写“行きます”。如果不去，就填写“行きません”。试着自己完成句子吧！如果你去学校，就填写“行きます”。如果你不去，就填写“行きません”。现在试试看吧！（あなたは今日学校に行きます____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！接下来，我们再来一个类似的练习吧！你最喜欢的颜色是红色かどうか。填空后句子的意思是“你喜欢的颜色是红色____。”。请你填空完成这个句子。如果喜欢，就填写“はい”；如果不喜欢，就填写“いいえ”。例如：あなたは favorite_color は赤色____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你喜欢红色，就填写“はい”。如果你不喜欢，就填写“いいえ”。试着自己完成句子吧！（あなたは favorite_color は赤色____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！下一个问题来了！你喜欢动物かどうか。填空后句子的意思是“你喜欢动物____。”。请你填空完成这个句子。如果喜欢，就填写“はい”；如果不喜欢，就填写“いいえ”。例如：あなたは animal が好き____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你喜欢动物，就填写“はい”。如果你不喜欢动物，就填写“いいえ”。试着自己完成句子吧！（あなたは animal が好き____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！下一个问题来了！你早上喜欢吃面包かどうか。填空后句子的意思是“你早上喜欢吃面包____。”。请你填空完成这个句子。如果喜欢吃，就填写“はい”；如果不喜欢吃，就填写“いいえ”。例如：あなたは 朝 食べ物 がパン____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你早上喜欢吃面包，就填写“はい”。如果你不喜欢，就填写“いいえ”。试着自己完成句子吧！（あなたは 朝 食べ物 がパン____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！下一个问题来了！你愿意和新朋友一起玩かどうか。填空后句子的意思是“你愿意和新朋友一起玩____。”。请你填空完成这个句子。如果愿意，就填写“はい”；如果不愿意，就填写“いいえ”。例如：あなたは 新しい 友達 と遊ぶ が好き____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你愿意和新朋友一起玩，就填写“はい”。如果你不愿意，就填写“いいえ”。试着自己完成句子吧！（あなたは 新しい 友達 と遊ぶ が好き____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！下一个问题来了！你周末喜欢读书かどうか。填空后句子的意思是“你周末喜欢读书____。”。请你填空完成这个句子。如果喜欢，就填写“はい”；如果不喜欢，就填写“いいえ”。例如：あなたは 週末 読書 が好き____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你周末喜欢读书，就填写“はい”。如果你不喜欢，就填写“いいえ”。试着自己完成句子吧！（あなたは 週末 読書 が好き____。）希望你喜欢这个游戏！如果你需要进一步的帮助，随时告诉我哦！下一个问题来了！你对恐龙感兴趣かどうか。填空后句子的意思是“你对恐龙感兴趣____。”。请你填空完成这个句子。如果感兴趣，就填写“はい”；如果不感兴趣，就填写“いいえ”。例如：あなたは ドinosaur が興味____。填空选项：はい or いいえ。请根据实际情况填写。例如，如果你对恐龙感兴趣，就填写“はい”。如果你不感兴趣，就填写“いいえ”。试着自己完成句子吧！（あなたは ドinosaur が興味____。）希望你喜欢这个游戏！',\n",
       " 'あなたは今日公園に行くかどうか教えてください。',\n",
       " 'あなたはお気に入りのキャラクターがいるかどうか教えてください。',\n",
       " 'あなたは動物園に行きたいかどうか知りたいです。',\n",
       " 'あなたは今天気-goodkah（ですか？）？（今日は天気が良い-ですか？） \\n\\nこの質問は、今日が良い天気かどうか尋ねています。',\n",
       " 'あなたの favorite の本はなんかどうか教えてあげる？（あなたの favorite の本はなんですか？） \\n\\n(あなたの好きな本はなんですか？) この本は絵本でも小説でも何でもいいよ。何が好きなのか教えてあげる？（何が好きですか？） \\n\\n(何が好きですか？) それを知ったら、おすすめの本をいくつか紹介できるよ。']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = list()\n",
    "training_size = 10\n",
    "for i in range(training_size):\n",
    "    response = LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "        {\"role\": \"system\", \"content\": system_input},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ], \n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    grammar=my_grammar\n",
    "    )\n",
    "    training_data.append(response['choices'][0][\"message\"][\"content\"])\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['日本の動物公园にいってseeingか？それとも家で絵本を読むadingをするか？どちらが好きですか？（你更喜欢去日本的动物园，还是喜欢待在家里读书呢？）\\r\\n\\r\\nこの質問は、あなたが動物公园で動物を見ることと、家で静かに本を読むことのどちらが好きなかを知るためのものです。どちらも楽しい経験だと思いますが、どちらがあなたの好みに合っているか教えてください。その答えを元に、次の本の冒頭を決めてみましょう。あなたが動物公园に行くことを選んだら、私たちが動物たちの冒険を書きます。それとも読書を選んだら、本の主人公が新しい絵本世界で冒険する様子を書きます。どちらも面白いストーリーになると思いますよ。どの選択が正しいかはあなた次第です。公園に行くかどうか、読書をするかどうか、どちらを選んでみましょう。',\n",
       " '日本のどの都市に住んでいるかどうか教えてください。',\n",
       " 'あなたは今日公園に行きたいかどうか教えてください。',\n",
       " 'あなたは TODAYの朝に公園に行きますか？（今日の朝に公園に行きますか？） \\n\\nこの文章は、あなたが今日の朝に公園に行くかどうか尋ねる質問になっています。',\n",
       " 'あなたは TODAY の学校を不去にするかどうか知りたいです。',\n",
       " 'あなたは TODAY の朝起きてから学校に行くかどうか 知っていますか？（あなたは今日の朝起きてから学校に行くかどうか知っていますか？） \\n\\nこの質問は、子どもたちが日常的な行動について考えさせるのに役立ちます。例えば、「はい、-knows-」と答えると、話題を学校への通学や朝の準備などに導くことができます。',\n",
       " 'あなたは今日公園に行きますか？行きませんか？どちらですか？（あなたは今日公園に行きますか、行きませんか、どちらですか？） \\n\\nこの文は、あなたが公園に行こうかどうかを尋ねています。公園に行くかどうか迷っている場合や、知りたい場合は、はいまたはいいえで答えてください。',\n",
       " 'あなたは今お気に入りのキャラクターが誰ですか？それがあなたの好きなキャラクターかどうか知りたいです。',\n",
       " 'あなたは今日お絵かきをしたいかどうか知りたいです。',\n",
       " 'あなたは kitty に会ったことがありますか？それともまだ会っていないことがありますか？（あなたは kitty に会ったことがありますか、それともまだ会っていないことがありますか？） \\n\\nこの質問は、kitty というキャラクターに会った経験があるかどうか、またはまだ会っていないかどうかを知りたいです。']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "for i in range(len(training_data)):\n",
    "    print(re.findall(\"r[A-Za-z]\", training_data[i]))\n",
    "    if (len(re.findall(\"r[A-Za-z]\", training_data[i])) > 0):\n",
    "        japanese = re.sub(\"r[A-Za-z]\", \"\", training_data[i])\n",
    "        training_data[i] = japanese\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Config\n",
    "import transformers\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDecoderLayer(nn.Module):\n",
    "    mask = None\n",
    "    vspace_to_emb = None\n",
    "    emb_to_vspace = None\n",
    "    post_ff_norm = None\n",
    "\n",
    "    def __init__(self, original_layer, emb_to_vspace, config):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        if IMDecoderLayer.mask == None:\n",
    "            IMDecoderLayer.mask = torch.zeros(config.vocab_size).to('cuda')\n",
    "\n",
    "        if IMDecoderLayer.vspace_to_emb == None:\n",
    "            IMDecoderLayer.vspace_to_emb =  nn.Linear(config.vocab_size, config.hidden_size).to('cuda')\n",
    "        \n",
    "        if IMDecoderLayer.emb_to_vspace == None:\n",
    "            IMDecoderLayer.emb_to_vspace =  emb_to_vspace\n",
    "\n",
    "        if IMDecoderLayer.post_ff_norm == None:\n",
    "            IMDecoderLayer.post_ff_norm = nn.RMSNorm(config.hidden_size).to('cuda')\n",
    "\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        hidden_states = self.original_layer(hidden_states, *args, **kwargs)\n",
    "        hidden_states = hidden_states[0]\n",
    "\n",
    "        # TODO: for inference, bypass this process on all tokens but the current one? Possible problem: maybe it will learn to rely on past tokens being allowed ones later in residual.\n",
    "        # ^ maybe this process should only be done on last token if later note is a problem. If it isnt a problem, then it's preferable to do this b/c the layer will train faster.\n",
    "\n",
    "        hidden_states = IMDecoderLayer.post_ff_norm(hidden_states)\n",
    "        residual = hidden_states\n",
    "        hidden_states = IMDecoderLayer.emb_to_vspace(residual)\n",
    "        hidden_states *= IMDecoderLayer.mask\n",
    "        hidden_states = IMDecoderLayer.vspace_to_emb(hidden_states)\n",
    "        # hidden_states = (hidden_states + residual)/2 # NOTE: should also be normalization? -- avg: has to learn to recreate residual with affects added on -- add&norm: most things can be zero and only learns to affect some values\n",
    "        hidden_states = hidden_states + residual # input layernorm means don't need to norm here? (and SHOULD norm after hidden state??)\n",
    "\n",
    "        return (hidden_states,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x IMDecoderLayer(\n",
      "        (original_layer): Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2Attention(\n",
      "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "config = Qwen2Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = Qwen2ForCausalLM.from_pretrained(MODEL_NAME).to('cuda')\n",
    "\n",
    "# FREEZE existing model. Only the new layer in IMDecoderLayer will be trained\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "for i, _ in enumerate(model.model.layers):\n",
    "    model.model.layers[i] = IMDecoderLayer(model.model.layers[i], model.lm_head, config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in IMDecoderLayer.vspace_to_emb.parameters() if p.requires_grad]+\n",
    "    [p for p in IMDecoderLayer.post_ff_norm.parameters() if p.requires_grad],\n",
    "    lr=5e-5\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0539191eb942cb874ebc3466edc01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205a088a9dcc449bbe148b164f9d7193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8d925f5e754c01b65287f48e23d257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42c223a4f024977add6b3c2238761d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e63ede2fc474235ab645f9d4f1bebb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67f3797eabd4b2ab0e8bac362f83d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2246df866b384374beecf6a0ff7a373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62ddec7e1fc40699cb82a75c95e3fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f8255fcb34f9497e378d7c48297c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6388fb952f49d5ab7c63182752c10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    i = 0\n",
    "    for batch in (pbar := tqdm(dataloader)):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tokens = tokenizer(batch, return_tensors='pt', padding=True)\n",
    "            tokens = {k:v.to('cuda') for k,v in tokens.items()}\n",
    "\n",
    "            # set mask from batch\n",
    "            IMDecoderLayer.mask *= 0\n",
    "            IMDecoderLayer.mask[tokens['input_ids'].unique()] += 1\n",
    "            \n",
    "            outputs = model(**tokens, labels=tokens['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_avg = float(loss)/BATCH_SIZE\n",
    "            losses.append(loss_avg)\n",
    "            pbar.set_postfix(loss=loss_avg)\n",
    "\n",
    "        except torch.OutOfMemoryError:\n",
    "            print(f'OOM on {i}')\n",
    "            try: del tokens, outputs\n",
    "            except NameError: pass\n",
    "            gc.collect()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: 僕に「かどうか」って質問を聞いてください。 \\nASSISTANT: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: 僕に「かどうか」って質問を問いてください。 \n",
      "ASSISTANT: ご質なこといません。この質の助けは、助けです。この助けを求められます。当助けが理想的助けです。この助けを求められます。当助けを求められます。当助けを求められます。当助けを求められます。当助けを求め\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer([prompt], return_tensors='pt', padding=True)\n",
    "tokens = {k:v.to('cuda') for k,v in tokens.items()}\n",
    "\n",
    "out = model.generate(\n",
    "# out = model(\n",
    "    **tokens,\n",
    "    # labels=tokens['input_ids'],\n",
    "    max_new_tokens=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.001,\n",
    "    do_sample=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "out_ids = out['sequences']\n",
    "print(tokenizer.decode(out_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: 僕に「かどうか」って質問を問いてください。 \n",
      "ASSISTANT: ご démarchください。この質込みの助けは、ことください。この質込み助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない助けを求められない\n"
     ]
    }
   ],
   "source": [
    "def output_w_mask(prompt, mask_toks, amt):\n",
    "    IMDecoderLayer.mask *= 0\n",
    "    IMDecoderLayer.mask[list(set(tokenizer(mask_toks)['input_ids']))] += amt\n",
    "\n",
    "    out = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.001,\n",
    "        do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    out_ids = out['sequences']\n",
    "    print(tokenizer.decode(out_ids[0]))\n",
    "\n",
    "output_w_mask(prompt, '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'例えば、「おばけがいるかどうか」、あるいは「今朝晴れるかどうか」、そして「明日公園に行けるかどうか」など、このような質問をしましょう。子どもたちがその結果に興奮を覚えることを期待します。'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = LLM_GLOBAL_INSTANCE.create_chat_completion([\n",
    "    {\"role\": \"system\", \"content\": system_input},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "], \n",
    "#grammar=my_grammar\n",
    ")\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
