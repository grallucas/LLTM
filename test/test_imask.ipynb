{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun -G1 --pty bash -c \"source /data/ai_club/team_3_2024-25/team3-env-finetune/bin/activate; \\\n",
    "    hostname; \\\n",
    "    jupyter notebook \\\n",
    "        --ServerApp.root_dir=$(pwd) \\\n",
    "        --ServerApp.password='' \\\n",
    "        --ServerApp.open_browser=False \\\n",
    "        --ServerApp.allow_origin='*' \\\n",
    "        --ServerApp.allow_remote_access=True \\\n",
    "        --ServerApp.port=14321 \\\n",
    "        --ServerApp.ip='*'\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:31:37.143611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 23:31:37.435675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744255897.555380 1160978 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744255897.590489 1160978 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 23:31:37.876111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Config\n",
    "import transformers\n",
    "# import json\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDecoderLayer(nn.Module):\n",
    "    mask = None\n",
    "    vspace_to_emb = None\n",
    "    emb_to_vspace = None\n",
    "    block_strength = []\n",
    "\n",
    "    scratch = None\n",
    "    norm = None\n",
    "\n",
    "    def __init__(self, original_layer, emb_to_vspace, vspace_to_emb, norm, config, block_idx):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        if IMDecoderLayer.vspace_to_emb == None:\n",
    "            IMDecoderLayer.vspace_to_emb = vspace_to_emb\n",
    "\n",
    "        if IMDecoderLayer.emb_to_vspace == None:\n",
    "            IMDecoderLayer.emb_to_vspace =  emb_to_vspace\n",
    "\n",
    "        if IMDecoderLayer.scratch == None:\n",
    "            IMDecoderLayer.scratch = torch.zeros(config.vocab_size, dtype=bool).to('cuda')\n",
    "\n",
    "        if IMDecoderLayer.norm == None:\n",
    "            IMDecoderLayer.norm = norm\n",
    "\n",
    "        self.block_idx = len(IMDecoderLayer.block_strength)\n",
    "        IMDecoderLayer.block_strength.append(\n",
    "            nn.Parameter(torch.tensor(1.0, dtype=torch.float32).to('cuda'))\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        hidden_states = self.original_layer(hidden_states, *args, **kwargs)\n",
    "        hidden_states = hidden_states[0]\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = IMDecoderLayer.emb_to_vspace(residual)\n",
    "        \n",
    "        assert IMDecoderLayer.mask != None\n",
    "\n",
    "        for i, positions in enumerate(IMDecoderLayer.mask):\n",
    "            for j, toks_allowed in enumerate(positions):\n",
    "                if not toks_allowed:\n",
    "                    hidden_states[i,j,:] = 0 \n",
    "                    continue\n",
    "\n",
    "                hidden_states[i,j,:] = 0 \n",
    "\n",
    "                IMDecoderLayer.scratch[:] = False\n",
    "                IMDecoderLayer.scratch[toks_allowed] = True\n",
    "                hidden_states[i,j,IMDecoderLayer.scratch] += 1/IMDecoderLayer.scratch.sum()\n",
    "\n",
    "                IMDecoderLayer.scratch[:] = True\n",
    "                IMDecoderLayer.scratch[toks_allowed] = False\n",
    "                hidden_states[i,j,IMDecoderLayer.scratch] -= 1/IMDecoderLayer.scratch.sum()\n",
    "\n",
    "        # print(hidden_states)\n",
    "        hidden_states = hidden_states @ IMDecoderLayer.vspace_to_emb.weight\n",
    "        hidden_states = hidden_states * IMDecoderLayer.block_strength[self.block_idx]\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        return (hidden_states,)\n",
    "\n",
    "def gen_mask(tokens):\n",
    "    mask = [] # mask[batch, position, allowed_tok]\n",
    "    for batch_size in tokens['attention_mask'].argmin(axis=1):\n",
    "        if batch_size == 0:\n",
    "            batch_size = tokens['attention_mask'].shape[1]\n",
    "        mask.append([[] for i in range(batch_size)])\n",
    "\n",
    "    return mask\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=3 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(do_masking):\n",
    "    MODEL_NAME = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "    config = Qwen2Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model = Qwen2ForCausalLM.from_pretrained(MODEL_NAME).to('cuda')\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    if do_masking:\n",
    "        # FREEZE existing model. Only the new layer in IMDecoderLayer will be trained\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # REPLACE transformer blocks with IM ones\n",
    "        for i, _ in enumerate(model.model.layers):\n",
    "            model.model.layers[i] = IMDecoderLayer(model.model.layers[i], model.lm_head, model.model.embed_tokens, model.model.norm, config, i)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding=True)\n",
    "        tokens = {k:v.to('cuda') for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "    def tokof(s, check=True):\n",
    "        toks = tokenizer(s, add_special_tokens=False)['input_ids']\n",
    "        if check:\n",
    "            if len(toks) > 1: raise Exception(f'This is more than one tok: {toks}')\n",
    "            return toks[0]\n",
    "        return toks\n",
    "\n",
    "    return model, tokenize, tokenizer, tokof\n",
    "\n",
    "model, tokenize, tokenizer, tokof = get_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\n",
    "    'terve', 'hei', 'talo', 'vesi', 'ystävä', 'huomenta', 'velho', 'suomi', 'koira', 'nimi', 'nimeni', 'nimesi', 'nimensä',\n",
    "    'ystäväni', 'ystäväsi', 'ystävänsä', 'vanha', 'hyvää', 'suomalainen', 'mukava', 'minä', 'minun', 'olen', 'olenko', 'sinä', 'sinun', 'olet',\n",
    "    'oletko', 'hän', 'hänen', 'on', 'onko', 'matti', 'aleksi', 'sami', 'kyllä', 'ei', 'mitä', 'mikä', 'kuka', 'rossi', 'lucas', '.', '!'\n",
    "]\n",
    "\n",
    "vocab += [v[0].upper() + v[1:] for v in vocab]\n",
    "vocab += [(' '+v if v.isalpha() else v) for v in vocab]\n",
    "# vocab += [v+'.' for v in vocab]\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "# --- BUILD DA TRIE ---\n",
    "\n",
    "trie = {}\n",
    "\n",
    "for v in vocab:\n",
    "    curr_node = trie\n",
    "\n",
    "    toks = tokof(v, check=False)\n",
    "\n",
    "    for tok in toks:\n",
    "        tok = tokenizer.decode(tok) # FOR VISUALIZING\n",
    "        if tok not in curr_node:\n",
    "            curr_node[tok] = {}\n",
    "        curr_node = curr_node[tok]\n",
    "\n",
    "    curr_node[None] = {}\n",
    "\n",
    "def get_next_allowed(given, trie):\n",
    "    allowed = trie\n",
    "    for tok in given:\n",
    "        # assume given already has valid seq\n",
    "        if tok in allowed:\n",
    "            allowed = allowed[tok]\n",
    "        elif None in allowed:\n",
    "            allowed = trie[tok]\n",
    "        else:\n",
    "            raise Exception(f'Unexpected token {tok}')\n",
    "\n",
    "    allowed = list(allowed.keys())\n",
    "\n",
    "    if None in allowed and given:\n",
    "        allowed += [t for t in trie.keys() if t[0] == ' ' or not t.isalpha()]\n",
    "\n",
    "    allowed = [v for v in allowed if v]\n",
    "\n",
    "    if not given:\n",
    "        allowed = [v for v in allowed if v[0] != ' ']\n",
    "\n",
    "    return allowed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'Sin', 'O', 'Van', 'o', 'N', 'Y', 'H', 'n', 'on', 'ei', 'hy', 'On', 'V', 'M', 'ross', 't', 'T', 'Mit', 'K', '!', 'olen', 'yst', 'l', 'Hy', 'ky', 'Su', 'sin', 'S', 'Min', 'su', 'min', 'E', '.', 'Vel', 'vel', 'h', 'k', 'Ale', 'hu', 'Hu', 'He', 'ale', 'Luc', 'Ross', 'van', 'hei', 'ko', 'ves', 'hä', 's', 'ter', 'mit']\n",
      "\n",
      "['let', 'len']\n",
      "\n",
      "['ko', ' hu', ' y', ' on', ' sin', ' K', ' vel', ' Hu', ' o', ' T', ' ei', ' Muk', ' h', ' N', ' On', ' k', ' n', '!', ' matt', ' V', ' su', ' Y', ' hei', ' m', ' hä', ' van', ' Sam', ' ale', ' ro', ' Lucas', ' O', ' E', ' mit', '.', ' Ky', ' Hä', ' ter', ' Su', ' mik', ' Sin', ' Min', ' Hy', ' tal', ' v', ' Ale', ' min', ' Mik', ' Ko', ' Mit', ' ko', ' Vel', ' luc', ' He', ' Van', ' Rossi', ' sam', ' H', ' Matt', ' hy']\n",
      "\n",
      "[' hu', ' y', ' on', ' sin', ' K', ' vel', ' Hu', ' o', ' T', ' ei', ' Muk', ' h', ' N', ' On', ' k', ' n', '!', ' matt', ' V', ' su', ' Y', ' hei', ' m', ' hä', ' van', ' Sam', ' ale', ' ro', ' Lucas', ' O', ' E', ' mit', '.', ' Ky', ' Hä', ' ter', ' Su', ' mik', ' Sin', ' Min', ' Hy', ' tal', ' v', ' Ale', ' min', ' Mik', ' Ko', ' Mit', ' ko', ' Vel', ' luc', ' He', ' Van', ' Rossi', ' sam', ' H', ' Matt', ' hy']\n",
      "\n",
      "['oment']\n",
      "\n",
      "['a']\n",
      "\n",
      "[' hu', ' y', ' on', ' sin', ' K', ' vel', ' Hu', ' o', ' T', ' ei', ' Muk', ' h', ' N', ' On', ' k', ' n', '!', ' matt', ' V', ' su', ' Y', ' hei', ' m', ' hä', ' van', ' Sam', ' ale', ' ro', ' Lucas', ' O', ' E', ' mit', '.', ' Ky', ' Hä', ' ter', ' Su', ' mik', ' Sin', ' Min', ' Hy', ' tal', ' v', ' Ale', ' min', ' Mik', ' Ko', ' Mit', ' ko', ' Vel', ' luc', ' He', ' Van', ' Rossi', ' sam', ' H', ' Matt', ' hy']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    get_next_allowed([], trie),\n",
    "    get_next_allowed([' o'], trie),\n",
    "    get_next_allowed([' o', 'len'], trie),\n",
    "    get_next_allowed([' o', 'len', 'ko'], trie),\n",
    "    get_next_allowed([' o', 'len', 'ko', ' hu'], trie),\n",
    "    get_next_allowed([' o', 'len', 'ko', ' hu', 'oment'], trie),\n",
    "    get_next_allowed([' o', 'len', 'ko', ' hu', 'oment', 'a'], trie),\n",
    "    sep='\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Finnish sentence: Onko sinulle tarkoitus, että tämän sinulla on sinu sinua? (Is it possible that this is the same person as you?) The correct answer is:\n",
      "\n",
      "A) Yes\n",
      "B) No\n",
      "\n",
      "C) It depends\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Here\\'s a Finnish sentence: Onko'\n",
    "\n",
    "print(prompt, end='')\n",
    "\n",
    "for _ in range(50):\n",
    "    tokens = tokenize(prompt)\n",
    "\n",
    "    # TODO: incorporate get_next_allowed into mask\n",
    "    # TODO: for now, only mask last token -- pros: simper mask, faster inference. cons: cant cannot parallelize training (doesn't matter for now)\n",
    "\n",
    "    IMDecoderLayer.mask = gen_mask(tokens)\n",
    "    IMDecoderLayer.mask[0][-1] += [tokof(' sin'), tokof('sin')]\n",
    "\n",
    "    out = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.000001,\n",
    "        # do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    tok = tokenizer.decode(out.sequences[0][-1])\n",
    "    prompt += tok\n",
    "    print(tok, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
