{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun -G1 --pty bash -c \"source /data/ai_club/team_3_2024-25/team3-env-finetune/bin/activate; \\\n",
    "    hostname; \\\n",
    "    jupyter notebook \\\n",
    "        --ServerApp.root_dir=$(pwd) \\\n",
    "        --ServerApp.password='' \\\n",
    "        --ServerApp.open_browser=False \\\n",
    "        --ServerApp.allow_origin='*' \\\n",
    "        --ServerApp.allow_remote_access=True \\\n",
    "        --ServerApp.port=14321 \\\n",
    "        --ServerApp.ip='*'\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 18:49:24.051897: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-11 18:49:24.064726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744411764.080614  997783 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744411764.085476  997783 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 18:49:24.101730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Config\n",
    "import transformers\n",
    "# import json\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDecoderLayer(nn.Module):\n",
    "    mask = None\n",
    "    vspace_to_emb = None\n",
    "    emb_to_vspace = None\n",
    "    block_strength = []\n",
    "\n",
    "    # scratch = None\n",
    "    # norm = None\n",
    "\n",
    "    def __init__(self, original_layer, emb_to_vspace, vspace_to_emb, norm, config, block_idx):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        if IMDecoderLayer.vspace_to_emb == None:\n",
    "            IMDecoderLayer.vspace_to_emb = vspace_to_emb\n",
    "\n",
    "        if IMDecoderLayer.emb_to_vspace == None:\n",
    "            IMDecoderLayer.emb_to_vspace =  emb_to_vspace\n",
    "\n",
    "        # if IMDecoderLayer.scratch == None:\n",
    "        #     IMDecoderLayer.scratch = torch.zeros(config.vocab_size, dtype=bool).to('cuda')\n",
    "\n",
    "        # if IMDecoderLayer.norm == None:\n",
    "        #     IMDecoderLayer.norm = norm\n",
    "\n",
    "        self.block_idx = len(IMDecoderLayer.block_strength)\n",
    "        IMDecoderLayer.block_strength.append(\n",
    "            nn.Parameter(torch.tensor(1.0, dtype=torch.float32).to('cuda'))\n",
    "        )\n",
    "\n",
    "        self.vstate = torch.zeros(config.vocab_size).to('cuda')\n",
    "\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        hidden_states = self.original_layer(hidden_states, *args, **kwargs)\n",
    "        hidden_states = hidden_states[0]\n",
    "\n",
    "        mask = IMDecoderLayer.mask\n",
    "        assert mask != None\n",
    "        if mask:\n",
    "            n_allowed = len(mask)\n",
    "            n_disallowed = self.vstate.shape[-1] - n_allowed\n",
    "\n",
    "            self.vstate *= 0\n",
    "            self.vstate[:] = -1/n_disallowed\n",
    "            self.vstate[mask] = 1/n_allowed\n",
    "            \n",
    "            hidden_states[-1,-1,:] += (self.vstate @ IMDecoderLayer.vspace_to_emb.weight) * IMDecoderLayer.block_strength[self.block_idx]\n",
    "\n",
    "        # residual = hidden_states\n",
    "        # hidden_states = IMDecoderLayer.emb_to_vspace(residual)\n",
    "        \n",
    "        # assert IMDecoderLayer.mask != None\n",
    "\n",
    "        # # for i, positions in enumerate(IMDecoderLayer.mask):\n",
    "        #     # for j, toks_allowed in enumerate(positions):\n",
    "        # toks_allowed = IMDecoderLayer.mask\n",
    "        # i, j = -1, -1\n",
    "\n",
    "        # if toks_allowed:\n",
    "        #     # hidden_states[i,j,:] = 0 \n",
    "\n",
    "        #     IMDecoderLayer.scratch[:] = False\n",
    "        #     IMDecoderLayer.scratch[toks_allowed] = True\n",
    "        #     hidden_states[i,j,IMDecoderLayer.scratch] += 1/IMDecoderLayer.scratch.sum()\n",
    "\n",
    "        #     IMDecoderLayer.scratch[:] = True\n",
    "        #     IMDecoderLayer.scratch[toks_allowed] = False\n",
    "        #     hidden_states[i,j,IMDecoderLayer.scratch] -= 1/IMDecoderLayer.scratch.sum()\n",
    "\n",
    "        # # print(hidden_states)\n",
    "        # hidden_states = hidden_states @ IMDecoderLayer.vspace_to_emb.weight\n",
    "        # hidden_states = hidden_states * IMDecoderLayer.block_strength[self.block_idx]\n",
    "        # hidden_states = hidden_states + residual\n",
    "\n",
    "        return (hidden_states,)\n",
    "\n",
    "def gen_mask(tokens):\n",
    "    # mask = [] # mask[batch, position, allowed_tok]\n",
    "    # for batch_size in tokens['attention_mask'].argmin(axis=1):\n",
    "    #     if batch_size == 0:\n",
    "    #         batch_size = tokens['attention_mask'].shape[1]\n",
    "    #     mask.append([[] for i in range(batch_size)])\n",
    "\n",
    "    mask = [] # mask[allowed_tok]\n",
    "\n",
    "    return mask\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=3 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(do_masking):\n",
    "    MODEL_NAME = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "    config = Qwen2Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    model = Qwen2ForCausalLM.from_pretrained(MODEL_NAME).to('cuda')\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    if do_masking:\n",
    "        # FREEZE existing model. Only the new layer in IMDecoderLayer will be trained\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # REPLACE transformer blocks with IM ones\n",
    "        for i, _ in enumerate(model.model.layers):\n",
    "            model.model.layers[i] = IMDecoderLayer(model.model.layers[i], model.lm_head, model.model.embed_tokens, model.model.norm, config, i)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding=True)\n",
    "        tokens = {k:v.to('cuda') for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "    def tokof(s, check=True):\n",
    "        toks = tokenizer(s, add_special_tokens=False)['input_ids']\n",
    "        if check:\n",
    "            if len(toks) > 1: raise Exception(f'This is more than one tok: {toks}')\n",
    "            return toks[0]\n",
    "        return toks\n",
    "\n",
    "    return model, tokenize, tokenizer, tokof\n",
    "\n",
    "model, tokenize, tokenizer, tokof = get_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = [\n",
    "#     '.', '!', ',', ':',\n",
    "#     'terve', 'hei', 'talo', 'vesi', 'ystävä', 'huomenta', 'velho', 'suomi', 'koira', 'nimi', 'nimeni', 'nimesi', 'nimensä',\n",
    "#     'ystäväni', 'ystäväsi', 'ystävänsä', 'vanha', 'hyvää', 'suomalainen', 'mukava', 'minä', 'minun', 'olen', 'olenko', 'sinä', 'sinun', 'olet',\n",
    "#     'oletko', 'hän', 'hänen', 'on', 'onko', 'matti', 'aleksi', 'sami', 'kyllä', 'ei', 'mitä', 'mikä', 'kuka', 'rossi', 'lucas'\n",
    "# ]\n",
    "\n",
    "vocab = [\n",
    "    '.', '!', ',', ':',\n",
    "    'hello', 'this', 'is', 'my', 'story', 'i', 'went', 'to', 'the', 'a', 'saw'\n",
    "    'car', 'store', 'park', 'and', 'wizard', 'saw', 'buy', 'buying', 'oranges', 'apples', 'stuff', 'good', 'wizards', 'am'\n",
    "]\n",
    "\n",
    "vocab_raw = vocab.copy()\n",
    "\n",
    "vocab += [v[0].upper() + v[1:] for v in vocab]\n",
    "vocab += [(' '+v if v.isalpha() else v) for v in vocab]\n",
    "# vocab += [v+'.' for v in vocab]\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "# --- BUILD DA TRIE ---\n",
    "\n",
    "trie = {}\n",
    "\n",
    "for v in vocab:\n",
    "    curr_node = trie\n",
    "\n",
    "    toks = tokof(v, check=False)\n",
    "\n",
    "    for tok in toks:\n",
    "        tok = tokenizer.decode(tok) # FOR VISUALIZING\n",
    "        if tok not in curr_node:\n",
    "            curr_node[tok] = {}\n",
    "        curr_node = curr_node[tok]\n",
    "\n",
    "    curr_node[None] = {}\n",
    "\n",
    "def get_next_allowed(given, trie):\n",
    "    allowed = trie\n",
    "    for tok in given:\n",
    "        if tok in allowed:\n",
    "            allowed = allowed[tok]\n",
    "        elif None in allowed and tok in trie:\n",
    "            allowed = trie[tok]\n",
    "        else:\n",
    "            # raise Exception(f'Unexpected token {tok}')\n",
    "            given = ['.'] # NOTE: fix for invalid prior seq - just pretend we're starting a new word\n",
    "\n",
    "    allowed = list(allowed.keys())\n",
    "\n",
    "    if None in allowed and given:\n",
    "        allowed += [t for t in trie.keys()]\n",
    "        # allowed += [t for t in trie.keys() if t[0] == ' ' or not t.isalpha()]\n",
    "\n",
    "    allowed = [v for v in allowed if v]\n",
    "\n",
    "    # if not given:\n",
    "    #     allowed = [v for v in allowed if v[0] != ' ']\n",
    "\n",
    "    return allowed\n",
    "    \n",
    "# print(\n",
    "    # get_next_allowed([], trie),\n",
    "    # get_next_allowed([' o'], trie),\n",
    "    # get_next_allowed([' o', 'len'], trie),\n",
    "    # get_next_allowed([' o', 'len', 'ko'], trie),\n",
    "    # get_next_allowed([' o', 'len', 'ko', ' hu'], trie),\n",
    "    # get_next_allowed([' o', 'len', 'ko', ' hu', 'oment'], trie),\n",
    "    # get_next_allowed([' o', 'len', 'ko', ' hu', 'oment', 'a'], trie),\n",
    "    # get_next_allowed([' on', 'ok', 'ko', ' O'], trie),\n",
    "#     sep='\\n\\n'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Your responses can only draw from this allowed vocab: ['.', '!', ',', ':', 'hello', 'this', 'is', 'my', 'story', 'i', 'went', 'to', 'the', 'a', 'sawcar', 'store', 'park', 'and', 'wizard', 'saw', 'buy', 'buying', 'oranges', 'apples', 'stuff', 'good', 'wizards', 'am'].<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me a short story with only the allowed vocab<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hello, this is my story, I went to the store, I saw a car, I saw a park, and I saw a wizard, I saw a wizard, I saw an orange, and an apple, and I bought oranges and apples,"
     ]
    }
   ],
   "source": [
    "# prompt = 'Here\\'s a Finnish sentence: Onko'\n",
    "prompt = f'''<|im_start|>system\n",
    "Your responses can only draw from this allowed vocab: {vocab_raw}.<|im_end|>\n",
    "<|im_start|>user\n",
    "Tell me a short story with only the allowed vocab<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=7 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))\n",
    "\n",
    "print(prompt, end='')\n",
    "\n",
    "for i in range(50):\n",
    "    tokens = tokenize(prompt)\n",
    "\n",
    "    # TODO: for now, only mask last token -- pros: simper mask, faster inference. cons: cant cannot parallelize training (doesn't matter for now)\n",
    "\n",
    "    given = [tokenizer.decode(t) for t in tokens['input_ids'][0]]\n",
    "    allowed = get_next_allowed(given, trie)\n",
    "    # print(allowed)\n",
    "    allowed = [tokof(t) for t in allowed] + [tokof('<|im_end|>')]\n",
    "\n",
    "    IMDecoderLayer.mask = gen_mask(tokens)\n",
    "    IMDecoderLayer.mask += allowed\n",
    "    # IMDecoderLayer.mask[0][-1] += allowed\n",
    "\n",
    "    out = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # temperature=0.2,\n",
    "        temperature=0.00001,\n",
    "        # do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # output_hidden_states=True\n",
    "        output_logits=True\n",
    "    )\n",
    "    \n",
    "    logits = out.logits[0][0]\n",
    "    logits[allowed] += 10/(i+1)**2\n",
    "    # tok_id = Categorical(logits=logits).sample() # temp = 1\n",
    "    tok_id = logits.argmax()\n",
    "    tok = tokenizer.decode(tok_id)\n",
    "\n",
    "    # tok = tokenizer.decode(out.sequences[0][-1])\n",
    "\n",
    "    prompt += tok\n",
    "    print(tok, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
