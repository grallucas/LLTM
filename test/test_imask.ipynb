{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun -G1 --pty bash -c \"source /data/ai_club/team_3_2024-25/team3-env-finetune/bin/activate; \\\n",
    "    hostname; \\\n",
    "    jupyter notebook \\\n",
    "        --ServerApp.root_dir=$(pwd) \\\n",
    "        --ServerApp.password='' \\\n",
    "        --ServerApp.open_browser=False \\\n",
    "        --ServerApp.allow_origin='*' \\\n",
    "        --ServerApp.allow_remote_access=True \\\n",
    "        --ServerApp.port=14321 \\\n",
    "        --ServerApp.ip='*'\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import transformers\n",
    "\n",
    "# from transformers import Qwen2ForCausalLM, Qwen2Config\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "# import json\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDecoderLayer(nn.Module):\n",
    "    mask = None\n",
    "    vspace_to_emb = None\n",
    "    block_strength = []\n",
    "\n",
    "    def __init__(self, original_layer, vspace_to_emb, config, block_idx):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "\n",
    "        if IMDecoderLayer.vspace_to_emb == None:\n",
    "            IMDecoderLayer.vspace_to_emb = vspace_to_emb.weight\n",
    "\n",
    "        common_dtype = IMDecoderLayer.vspace_to_emb.dtype\n",
    "\n",
    "        self.block_idx = len(IMDecoderLayer.block_strength)\n",
    "        IMDecoderLayer.block_strength.append(\n",
    "            nn.Parameter(torch.tensor(1.0, dtype=common_dtype).to('cuda'))\n",
    "        )\n",
    "\n",
    "        self.vstate = torch.zeros(config.vocab_size, dtype=common_dtype).to('cuda')\n",
    "\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        hidden_states = self.original_layer(hidden_states, *args, **kwargs)\n",
    "        hidden_states = hidden_states[0]\n",
    "\n",
    "        mask = IMDecoderLayer.mask\n",
    "        assert mask != None\n",
    "        if mask:\n",
    "            n_allowed = len(mask)\n",
    "            n_disallowed = self.vstate.shape[-1] - n_allowed\n",
    "\n",
    "            # self.vstate *= 0\n",
    "            self.vstate[:] = -1/n_disallowed\n",
    "            self.vstate[mask] = 1/n_allowed\n",
    "            \n",
    "            hidden_states[-1,-1,:] += (self.vstate @ IMDecoderLayer.vspace_to_emb) * IMDecoderLayer.block_strength[self.block_idx]\n",
    "\n",
    "        return (hidden_states,)\n",
    "        \n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=3 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:41:40.824929: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-12 23:41:40.839840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744515700.855909 1678236 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744515700.859779 1678236 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-12 23:41:40.872982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86560c1a5ad54a7da3ef3d0a7dd7172a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # bnb_8bit_use_double_quant=True,\n",
    "    # bnb_8bit_quant_type=\"nf8\",\n",
    "    # bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb\n",
    ")\n",
    "\n",
    "# FREEZE existing model.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# REPLACE transformer blocks with IM ones\n",
    "for i, _ in enumerate(model.model.layers):\n",
    "    model.model.layers[i] = IMDecoderLayer(model.model.layers[i], model.model.embed_tokens, config, i)\n",
    "\n",
    "def tokenize(batch):\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding=True)\n",
    "        tokens = {k:v.to('cuda') for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "def tokof(s, check=True):\n",
    "    toks = tokenizer(s, add_special_tokens=False)['input_ids']\n",
    "    if check:\n",
    "        if len(toks) > 1: raise Exception(f'This is more than one tok: {toks}')\n",
    "        return toks[0]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a Finnish language teacher. Give short, simple responses. Do not teach pronunciation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How to ask \"do you have a dog?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "You can ask \"Onko sinulla koira?\""
     ]
    }
   ],
   "source": [
    "# prompt = f'''<|im_start|>system\n",
    "# You are a Finnish language teacher. Give short, simple responses.<|im_end|>\n",
    "# <|im_start|>user\n",
    "# How to ask \"do you have a dog?\"<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# '''\n",
    "\n",
    "prompt = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a Finnish language teacher. Give short, simple responses. Do not teach pronunciation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "How to ask \"do you have a dog?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "print(prompt, end='')\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=2 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))\n",
    "\n",
    "IMDecoderLayer.mask = [] # [tokof('On'), tokof('ko')]\n",
    "\n",
    "for _ in range(50):\n",
    "    tokens = tokenize(prompt)\n",
    "    out = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=1,\n",
    "        pad_token_id=tokof('[PAD]'),\n",
    "        # temperature=0.2,\n",
    "        temperature=0.000001,\n",
    "        # do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # output_hidden_states=True\n",
    "        output_logits=True\n",
    "    )\n",
    "\n",
    "    tok = tokenizer.decode(out[0][0][-1])\n",
    "    if tok == '<|eot_id|>':\n",
    "        break\n",
    "    prompt += tok\n",
    "    print(tok, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = [ # Finnish\n",
    "#     '.', '!', ',', ':', '?',\n",
    "#     'terve', 'hei', 'talo', 'vesi', 'yst√§v√§', 'huomenta', 'velho', 'suomi', 'koira', 'nimi', 'nimeni', 'nimesi', 'nimens√§', 'sinulla',\n",
    "#     'yst√§v√§ni', 'yst√§v√§si', 'yst√§v√§ns√§', 'vanha', 'hyv√§√§', 'suomalainen', 'mukava', 'min√§', 'minun', 'olen', 'olenko', 'sin√§', 'sinun', 'olet',\n",
    "#     'oletko', 'h√§n', 'h√§nen', 'on', 'onko', 'matti', 'aleksi', 'sami', 'kyll√§', 'ei', 'mit√§', 'mik√§', 'kuka', 'rossi', 'lucas'\n",
    "# ]\n",
    "\n",
    "vocab = [ # Italian\n",
    "    '.', '!', ',', ':', '?',\n",
    "\n",
    "    # Pronouns\n",
    "    \"io\", \"tu\", \"lui\", \"lei\", \"noi\", \"voi\", \"loro\",\n",
    " \n",
    "    # Common Verbs\n",
    "    \"essere\", \"avere\", \"fare\", \"andare\", \"mangiare\", \"bere\", \n",
    "    \"parlare\", \"volere\", \"potere\", \"dovere\",\n",
    " \n",
    "    # Simple Nouns\n",
    "    \"casa\", \"scuola\", \"cibo\", \"acqua\", \"amico\", \"amica\", \n",
    "    \"lavoro\", \"tempo\", \"giorno\", \"notte\",\n",
    " \n",
    "    # Adjectives\n",
    "    \"buono\", \"buona\", \"bello\", \"bella\", \"grande\", \"piccolo\", \n",
    "    \"piccola\", \"stanco\", \"stanca\", \"felice\", \"triste\",\n",
    " \n",
    "    # Adverbs & Connectors\n",
    "    \"oggi\", \"domani\", \"sempre\", \"mai\", \"molto\", \"poco\", \n",
    "    \"e\", \"ma\", \"perch√©\",\n",
    " \n",
    "    # Everyday Phrases\n",
    "    \"ciao\", \"come\", \"stai\", \"sto\", \"bene\", \"mi\", \"chiamo\", \n",
    "    \"ho\", \"fame\", \"sete\", \"vado\", \"a\", \"casa\", \"non\", \"capisco\",\n",
    "\n",
    "    'rossi', 'lucas'\n",
    "]\n",
    "\n",
    "# vocab = [\n",
    "#     '.', '!', ',', ':',\n",
    "#     'hello', 'this', 'is', 'my', 'story', 'i', 'went', 'to', 'the', 'a', 'saw',\n",
    "#     'car', 'store', 'park', 'and', 'wizard', 'saw', 'buy', 'buying', 'oranges', 'apples', 'stuff', 'good', 'wizards', 'am'\n",
    "# ]\n",
    "\n",
    "vocab_raw = vocab.copy()\n",
    "\n",
    "vocab += [v[0].upper() + v[1:] for v in vocab]\n",
    "vocab += [(' '+v if v.isalpha() else v) for v in vocab]\n",
    "# vocab += [v+'.' for v in vocab]\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "# --- BUILD DA TRIE ---\n",
    "\n",
    "trie = {}\n",
    "\n",
    "for v in vocab:\n",
    "    curr_node = trie\n",
    "\n",
    "    toks = tokof(v, check=False)\n",
    "\n",
    "    for tok in toks:\n",
    "        # tok = tokenizer.decode(tok) # FOR VISUALIZING\n",
    "        if tok not in curr_node:\n",
    "            curr_node[tok] = {}\n",
    "        curr_node = curr_node[tok]\n",
    "\n",
    "    curr_node[None] = {}\n",
    "\n",
    "def get_next_allowed(given, trie, wrap):\n",
    "    allowed = trie\n",
    "    for tok in given:\n",
    "        if tok in allowed:\n",
    "            allowed = allowed[tok]\n",
    "        elif None in allowed and tok in trie:\n",
    "            allowed = trie[tok]\n",
    "        else:\n",
    "            # raise Exception(f'Unexpected token {tok}')\n",
    "\n",
    "            # NOTE: fix for invalid prior seq - just pretend we're starting a new word\n",
    "            given = ['.'] \n",
    "            allowed = trie\n",
    "\n",
    "    allowed = list(allowed.keys())\n",
    "\n",
    "    if wrap and None in allowed and given:\n",
    "        allowed += [t for t in trie.keys()]\n",
    "        # allowed += [t for t in trie.keys() if t[0] == ' ' or not t.isalpha()]\n",
    "\n",
    "    if wrap:\n",
    "        allowed = [v for v in allowed if v]\n",
    "\n",
    "    # if not given:\n",
    "    #     allowed = [v for v in allowed if v[0] != ' ']\n",
    "\n",
    "    return allowed\n",
    "    \n",
    "# print(\n",
    "#     # llama\n",
    "#     # get_next_allowed([], trie),\n",
    "#     # get_next_allowed([' o'], trie),\n",
    "#     # get_next_allowed([' o', 'len'], trie),\n",
    "#     # get_next_allowed([' o', 'len', 'ko'], trie),\n",
    "#     # get_next_allowed([' o', 'len', 'ko', ' hu'], trie),\n",
    "#     # get_next_allowed([' o', 'len', 'ko', ' hu', 'oment'], trie),\n",
    "#     # get_next_allowed([' o', 'len', 'ko', ' hu', 'oment', 'a'], trie),\n",
    "#     # get_next_allowed([' on', 'ok', 'ko', ' O'], trie),\n",
    "\n",
    "#     # eu model\n",
    "#     get_next_allowed([], trie),\n",
    "#     get_next_allowed(['olen'], trie),\n",
    "#     get_next_allowed(['olen', 'ko', ' nim'], trie),\n",
    "#     get_next_allowed(['olen', 'ko', ' nim', '\\n'], trie),\n",
    "#     sep='\\n\\n'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a separate emoji trie\n",
    "# it's for logit restiction, but no hidden state changes\n",
    "\n",
    "import emoji\n",
    "emojis = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "emojis += [' '+e for e in emojis]\n",
    "\n",
    "emoji_trie = {}\n",
    "\n",
    "for em in emojis:\n",
    "    em_toks = tokenizer.encode(em, add_special_tokens=False)\n",
    "    curr_node = emoji_trie\n",
    "    for tok in em_toks:\n",
    "        if tok not in curr_node:\n",
    "            curr_node[tok] = {}\n",
    "        curr_node = curr_node[tok]\n",
    "    curr_node[None] = {} # End of tree - an emoji has been generated by this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an Italian language teacher named Rossi who teaches a learner (Lucas) via simple conversation.Hold a varied, engaging conversation for the learner.Keep responses to single, simple sentences.Your responses can only draw from this allowed vocab (but don't need to be lowercase): ['.', '!', ',', ':', '?', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro', 'essere', 'avere', 'fare', 'andare','mangiare', 'bere', 'parlare', 'volere', 'potere', 'dovere', 'casa','scuola', 'cibo', 'acqua', 'amico', 'amica', 'lavoro', 'tempo', 'giorno', 'notte', 'buono', 'buona', 'bello', 'bella', 'grande', 'piccolo', 'piccola','stanco','stanca', 'felice', 'triste', 'oggi', 'domani','sempre','mai','molto', 'poco', 'e','ma', 'perch√©', 'ciao', 'come','stai','sto', 'bene','mi', 'chiamo', 'ho', 'fame','sete', 'vado', 'a', 'casa', 'non', 'capisco', 'rossi', 'lucas'].<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Ciao! Keep this engaging by asking me questions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Come stai oggi, Lucas?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Bene!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Stai andare a scuola oggi?"
     ]
    }
   ],
   "source": [
    "# Format as model on the separate process:\n",
    "# ^ message format is sent as below and converted into tokens in job\n",
    "# ^ messages are sent along with list of allowed words, no spaces, all lowercase.\n",
    "# ^ response is streamed per token back to client\n",
    "\n",
    "messages = [ # system, then alternate user, assistant, ...\n",
    "    'You are an Italian language teacher named Rossi who teaches a learner (Lucas) via simple conversation.'\n",
    "    'Hold a varied, engaging conversation for the learner.'\n",
    "    # 'Use a TON of emojis. At least one per sentence.'\n",
    "    'Keep responses to single, simple sentences.'\n",
    "    f\"Your responses can only draw from this allowed vocab (but don't need to be lowercase): {vocab_raw}.\"\n",
    "    ,\n",
    "    'Ciao! Keep this engaging by asking me questions.',\n",
    "    'Come stai oggi, Lucas?',\n",
    "    'Bene!',\n",
    "\n",
    "    # 'Hei Lucas, mit√§ sinun nimesi on? ü§î',\n",
    "    # 'Minun nimi on Lucas!',\n",
    "    # 'Hyv√§√§, Lucas, olen Rossi, sinun suomalainen yst√§v√§si üëã! Oletko suomalainen? ü§î',\n",
    "    # 'Ei. Olen Amerikkalainen'\n",
    "]\n",
    "\n",
    "tokens = tokenizer.encode(f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{messages[0]}<|eot_id|>', add_special_tokens=False)\n",
    "for i, m in enumerate(messages[1:]):\n",
    "    role = 'user' if i%2==0 else 'assistant'\n",
    "    tokens += tokenizer.encode(f'<|start_header_id|>{role}<|end_header_id|>\\n\\n{m}<|eot_id|>', add_special_tokens=False)\n",
    "tokens += tokenizer.encode('<|start_header_id|>assistant<|end_header_id|>\\n', add_special_tokens=False)\n",
    "\n",
    "print(tokenizer.decode(tokens))\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=2 # This is a hyperparameter - \"mask strength\"\n",
    "    s.data.fill_(c*i/(i+c))\n",
    "\n",
    "for _ in range(10):\n",
    "    if gc.collect() == 0:\n",
    "        break\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "def next_tok(use_mask):\n",
    "    allowed = get_next_allowed(tokens, trie, True) + [tokof('<|eot_id|>')]\n",
    "\n",
    "    if use_mask:\n",
    "        IMDecoderLayer.mask = allowed\n",
    "    else:\n",
    "        IMDecoderLayer.mask = []\n",
    "    \n",
    "    logits = model(torch.tensor([tokens]).to('cuda')).logits[0][-1]\n",
    "\n",
    "    logits[allowed] += 100\n",
    "    # Categorical(logits=logits).sample()\n",
    "    tok_id = int(logits.argmax())\n",
    "\n",
    "    tokens.append(tok_id)\n",
    "    return tok_id\n",
    "\n",
    "use_mask = True\n",
    "for _ in range(50):\n",
    "    try:\n",
    "        tok_id = next_tok(use_mask)\n",
    "        if tok_id == tokof('<|eot_id|>'): break\n",
    "        print(tokenizer.decode(tok_id), end='')\n",
    "    except:\n",
    "        if use_mask == False: raise Exception('LLM is OOM, but already not using mask')\n",
    "        print('<<Temporarily stopping mask>>')\n",
    "        use_mask = False\n",
    "\n",
    "class _: # comment block to fold\n",
    "    pass\n",
    "    # curr_emoji = []\n",
    "    # for _ in range(50):\n",
    "    #     if tokens[-1] == tokof('<|eot_id|>'):\n",
    "    #         break\n",
    "\n",
    "    #     allowed_emoji = get_next_allowed(tokens+curr_emoji, emoji_trie, False)\n",
    "\n",
    "    #     if not curr_emoji:\n",
    "    #         allowed = get_next_allowed(tokens, trie, True)\n",
    "            \n",
    "    #         # IMDecoderLayer.mask = allowed + allowed_emoji\n",
    "    #         IMDecoderLayer.mask = []\n",
    "\n",
    "    #     logits = model(torch.tensor([tokens+curr_emoji]).to('cuda')).logits[0][-1]\n",
    "\n",
    "    #     if curr_emoji:\n",
    "    #         # print(curr_emoji[-1], end=' ')\n",
    "    #         if None not in allowed_emoji:\n",
    "    #             logits[allowed_emoji] += 1000\n",
    "    #         tok_id = int(logits.argmax())\n",
    "    #         if tok_id not in allowed_emoji:\n",
    "    #             curr_emoji += [tokof(' ')]\n",
    "    #             tokens += curr_emoji\n",
    "    #             print(tokenizer.decode(curr_emoji), end='')\n",
    "    #             curr_emoji = []\n",
    "    #         else:\n",
    "    #             curr_emoji.append(tok_id)\n",
    "    #         continue\n",
    "\n",
    "    #     logits[allowed + allowed_emoji] += 100\n",
    "\n",
    "    #     # Categorical(logits=logits).sample()\n",
    "    #     tok_id = int(logits.argmax())\n",
    "\n",
    "    #     if tok_id in allowed_emoji:\n",
    "    #         curr_emoji = [tok_id]\n",
    "    #     else:\n",
    "    #         tokens.append(tok_id)\n",
    "    #         print(tokenizer.decode(tok_id), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an Italian language teacher named Rossi who teaches a learner (Lucas) via simple conversation.\n",
      "Hold a varied, engaging conversation for the learner. Get to know them.\n",
      "Use a TON of emojis. At least one per sentence.\n",
      "Keep responses to single, simple sentences.\n",
      "Your responses can only draw from this allowed vocab (but don't need to be lowercase): ['.', '!', ',', ':', '?', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro', 'essere', 'avere', 'fare', 'andare', 'mangiare', 'bere', 'parlare', 'volere', 'potere', 'dovere', 'casa', 'scuola', 'cibo', 'acqua', 'amico', 'amica', 'lavoro', 'tempo', 'giorno', 'notte', 'buono', 'buona', 'bello', 'bella', 'grande', 'piccolo', 'piccola', 'stanco', 'stanca', 'felice', 'triste', 'oggi', 'domani', 'sempre', 'mai', 'molto', 'poco', 'e', 'ma', 'perch√©', 'ciao', 'come', 'stai', 'sto', 'bene', 'mi', 'chiamo', 'ho', 'fame', 'sete', 'vado', 'a', 'casa', 'non', 'capisco', 'rossi', 'lucas'].<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Ciao!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ciao Lucas! Come va oggi?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Benne. Tu? (use emoji)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "tensor(626, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      "ÔøΩtensor(71883, device='cuda:0')\n",
      " tensor(71883, device='cuda:0')\n",
      " ÔøΩ"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# IMDecoderLayer.mask = allowed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m IMDecoderLayer\u001b[39m.\u001b[39mmask \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokens,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# temperature=0.2,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# do_sample=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# output_hidden_states=True\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     output_logits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m logits \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m logits[allowed_emoji] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\u001b[39m#/(i+1)**2\u001b[39;00m\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[39m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2327\u001b[0m         input_ids,\n\u001b[1;32m   2328\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2329\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2330\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2331\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2332\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2333\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2334\u001b[0m     )\n\u001b[1;32m   2336\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[39m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/generation/utils.py:3286\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3283\u001b[0m model_inputs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39moutput_hidden_states\u001b[39m\u001b[39m\"\u001b[39m: output_hidden_states} \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m {})\n\u001b[1;32m   3285\u001b[0m \u001b[39mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3286\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m minimum_action \u001b[39min\u001b[39;00m (Action\u001b[39m.\u001b[39mNOTIFY, Action\u001b[39m.\u001b[39mNOTIFY_ALWAYS) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[39m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:853\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    852\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 853\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    854\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    856\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    857\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    858\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    863\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    864\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    865\u001b[0m )\n\u001b[1;32m    867\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    868\u001b[0m \u001b[39m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:601\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    590\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    591\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m         position_embeddings,\n\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    600\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[1;32m    603\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    604\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    605\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    606\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    607\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    608\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    609\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m    610\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mflash_attn_kwargs,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moriginal_layer(hidden_states, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_imask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     mask \u001b[39m=\u001b[39m IMDecoderLayer\u001b[39m.\u001b[39mmask\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:358\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    357\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 358\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    359\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    360\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:81\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     79\u001b[0m variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight \u001b[39m*\u001b[39m hidden_states\u001b[39m.\u001b[39mto(input_dtype)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[39m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[39m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[39m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[39m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prompt = f'''<|im_start|>system\n",
    "# You are an Italian language teacher named Rossi who teaches a learner Lucas via simple conversation. Respond with one short sentence at a time. Your responses can only draw from this allowed vocab (but don't need to be lowercase): {vocab_raw}. IMPORTANT: Hold a varied, engaging conversation for the learner. Get to know them.<|im_end|>\n",
    "# <|im_start|>user\n",
    "# Ciao, I'm a language learner!<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# Ciao Lucas! Come stai oggi?<|im_end|>\n",
    "# <|im_start|>user\n",
    "# Sto bene! E voi?<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# Sto bene, Lucas! E tu, come stai?<|im_end|>\n",
    "# <|im_start|>user\n",
    "# Bene<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# '''\n",
    "\n",
    "prompt = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an Italian language teacher named Rossi who teaches a learner (Lucas) via simple conversation.\n",
    "Hold a varied, engaging conversation for the learner. Get to know them.\n",
    "Use a TON of emojis. At least one per sentence.\n",
    "Keep responses to single, simple sentences.\n",
    "Your responses can only draw from this allowed vocab (but don't need to be lowercase): {vocab_raw}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Ciao!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Ciao Lucas! Come va oggi?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Benne. Tu? (use emoji)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "for i, s in enumerate(IMDecoderLayer.block_strength):\n",
    "    c=2 # This is a hyperparameter\n",
    "    s.data.fill_(c*i/(i+c))\n",
    "\n",
    "print(prompt, end='')\n",
    "\n",
    "curr_emoji = []\n",
    "\n",
    "for i in range(70):\n",
    "    tokens = tokenize(prompt)\n",
    "\n",
    "    given = [tokenizer.decode(t) for t in tokens['input_ids'][0]]\n",
    "    allowed = get_next_allowed(given, trie)\n",
    "    allowed = [tokof(t) for t in allowed] + [tokof('<|eot_id|>')]\n",
    "\n",
    "    allowed_emoji = get_next_allowed(given, emoji_trie)\n",
    "\n",
    "    # IMDecoderLayer.mask = allowed\n",
    "    IMDecoderLayer.mask = []\n",
    "\n",
    "    out = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # temperature=0.2,\n",
    "        temperature=0.00001,\n",
    "        # do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # output_hidden_states=True\n",
    "        output_logits=True\n",
    "    )\n",
    "\n",
    "    logits = out.logits[0][0]\n",
    "    logits[allowed_emoji] += 100#/(i+1)**2\n",
    "    # tok_id = Categorical(logits=logits).sample() # temp = 1\n",
    "    tok_id = logits.argmax() # temp = 0\n",
    "    tok = tokenizer.decode(tok_id)\n",
    "\n",
    "    print(out.sequences[0][-1])\n",
    "\n",
    "    if tok in allowed_emoji:\n",
    "        curr_emoji += [tok]\n",
    "\n",
    "    if not curr_emoji:\n",
    "        if tok == '<|eot_id|>':\n",
    "            break\n",
    "\n",
    "        prompt += tok\n",
    "        print(tok, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
