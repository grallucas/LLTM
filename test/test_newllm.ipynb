{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --job-name=LLTM -G1 --pty \\\n",
    "    bash -c \"source /data/ai_club/team_3_2024-25/team3-env-finetune/bin/activate; \\\n",
    "    hostname; \\\n",
    "    jupyter notebook \\\n",
    "        --ServerApp.root_dir=$(pwd) \\\n",
    "        --ServerApp.password='' \\\n",
    "        --ServerApp.open_browser=False \\\n",
    "        --ServerApp.allow_origin='*' \\\n",
    "        --ServerApp.allow_remote_access=True \\\n",
    "        --ServerApp.port=14321 \\\n",
    "        --ServerApp.ip='*'\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 23:56:00.631194: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 23:56:00.644262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742788560.660596 1392256 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742788560.665639 1392256 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 23:56:00.681754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ac249cc70649debbf9dd319d388486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/756 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1b331299104f8085cb66606518072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/31.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979534d00bac4da39db941986a9c8a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67689e879ff41ebb367afcffcd316fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac01a430f8a459bba73e65284fe8466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab55bdfa92d42448aa6fbdcf442c7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbce894633ac4c6cb728952d0aaddeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66bb367f1704b7da8fcdb5eb0c0833d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89636680160a49f2a6dd908d4827392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"microsoft/Phi-4-mini-instruct\" # ms claims it knows finnish, but FAKE!!!\n",
    "model_id = \"utter-project/EuroLLM-9B-Instruct\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # bnb_8bit_use_double_quant=True,\n",
    "    # bnb_8bit_quant_type=\"nf8\",\n",
    "    # bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# TOKEN = ''\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=TOKEN\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oletko koskaan ajatellut, että joku muu voisi nähdä'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(\n",
    "        **tokenizer('Oletko', return_tensors='pt').to('cuda'),\n",
    "        max_new_tokens=10\n",
    "    )[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\n",
    "    'terve', 'hei', 'talo', 'vesi', 'ystävä', 'huomenta', 'velho', 'suomi', 'koira', 'nimi', 'nimeni', 'nimesi', 'nimensä',\n",
    "    'ystäväni', 'ystäväsi', 'ystävänsä', 'vanha', 'hyvää', 'suomalainen', 'mukava', 'minä', 'minun', 'olen', 'olenko', 'sinä', 'sinun', 'olet',\n",
    "    'oletko', 'hän', 'hänen', 'on', 'onko', 'matti', 'aleksi', 'sami', 'kyllä', 'ei', 'mitä', 'mikä', 'kuka', 'rossi', 'lucas'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/data/ai_club/team_3_2024-25/team3-env-finetune/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Tässä lauseessa ei ole virheitä. Se on oikein muodostettu ja selkeä. Käännös on myös oikein: \"This is my dog.\"\\n\\nJos haluat tehdä lauseesta hieman monipuolisemman, voit lisätä siihen lisää yksityiskohtia, kuten koiran nimen tai ikää. Esimerkiksi: \"Tämä on minun koirani, jonka nimi on Max ja joka on 3-vuotias.\"'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    " \n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "pipe(\n",
    "    [\n",
    "        # {\"role\": \"system\", \"content\": f\"You are a Finnish language teacher.\\n\\n'IMPORTANT: Your responses must only use words in this allowed vocab: {vocab} and any emoji/punctuation.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Use the words in the allowed vocab to ask if I have something\"}\n",
    "\n",
    "        # {\"role\": \"system\", \"content\": f\"You are a Finnish language teacher.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"What are the mistakes in this sentence:\\nSyön jäätelö\"}\n",
    "      \n",
    "        # {\"role\": \"system\", \"content\": f\"You are a Finnish language teacher.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"What are the mistakes in this sentence:\\nTämä on minun koirasi\"}\n",
    "\n",
    "        {\"role\": \"system\", \"content\": f\"Sinä olet suomen kielen opettaja.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Mitkä ovat tämän lauseen virheet, jos niitä on?\\nTämä on minun koirasi\"}\n",
    "    ],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"correct_sentence\": \"Tämä on minun koirani\", \"breif explanation\": \"NA\"}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " "
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "    api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "out = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a Finnish teacher.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the most obvious mistake in this sentence (if it has one) \\\"Tämä on minun koirani\\\"? Respond in JSON {'correct_sentence': str, 'breif explanation': str|'NA'}.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    "    temperature=0,\n",
    "    response_format={'type': 'json_object'}\n",
    ")\n",
    "\n",
    "out_str = ''\n",
    "for t in out:\n",
    "    tok = t.choices[0].delta.content\n",
    "    if not tok: continue\n",
    "    print(tok, end='')\n",
    "    out_str += tok\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 2 (char 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m json\u001b[39m.\u001b[39;49mloads(out_str\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m}\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-conda-py312-glibc/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-conda-py312-glibc/lib/python3.12/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 2 (char 84)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.loads(out_str+'}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chat-224c2af44cd046ba8a2f4706dc524330', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='In Finnish, the correct sentence would be \"Tämä on minun koirani\" actually has one mistake that is quite common for non-native speakers.\\n\\nThe mistake is the use of \"minun\" instead of \"omani\". \"Minun\" is the genitive form of the pronoun \"minä\", whereas \"omani\" is the possessive form.\\n\\nSo, the corrected sentence would be: \"Tämä on omani koira.\"\\n\\nHowever, it\\'s worth noting', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None), stop_reason=None)], created=1742499135, model='meta/llama-3.1-70b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=48, total_tokens=148, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a Finnish teacher.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the most obvious mistake in this sentence (if it has one) \\\"Tämä on minun koirani\\\"?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Finnish, the correct sentence would be \"Tämä on minun koirani\" actually has one mistake that is quite common for non-native speakers.\\n\\nThe mistake is the use of \"minun\" instead of \"omani\". \"Minun\" is the genitive form of the pronoun \"minä\", whereas \"omani\" is the possessive form.\\n\\nSo, the corrected sentence would be: \"Tämä on omani koira.\"\\n\\nHowever, it\\'s worth noting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m enc\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mYou are a Finnish teacher.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m \u001b[39m# 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "enc.encode(\"You are a Finnish teacher.\").__len__() + 4 # 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import getpass\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "USER = getpass.getuser()\n",
    "\n",
    "TOKEN_COUNT_PATH = None\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "    api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# TODO REMOVE\n",
    "TOKEN_COUNT_PATH = '/data/ai_club/team_3_2024-25/tokcounts2/'\n",
    "\n",
    "def _ntoks(text):\n",
    "    return enc.encode(text).__len__() + 4 # 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format)\n",
    "\n",
    "def _inc_tok_count(mode, amt):\n",
    "    if TOKEN_COUNT_PATH is None:\n",
    "        raise Exception('Set TOKEN_COUNT_PATH before infernece')\n",
    "    fname = f'{USER}_{mode}.txt'\n",
    "    try:\n",
    "        with open(TOKEN_COUNT_PATH+fname, 'r') as f:\n",
    "            count = int(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        count = 0\n",
    "    except ValueError:\n",
    "        raise Exception(f'Token Count Corrupted: {fname}')\n",
    "    \n",
    "    count += amt\n",
    "\n",
    "    with open(TOKEN_COUNT_PATH+fname, 'w') as f:\n",
    "        f.write(str(count)+'\\n')\n",
    "\n",
    "@dataclass\n",
    "class Msg:\n",
    "    role: str\n",
    "    content: str\n",
    "    response_format: list = None\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, sys_prompt:str=None):\n",
    "        self._hist = []\n",
    "        self._awaiting_streamed = False\n",
    "\n",
    "    def _hist_to_prompt(self):\n",
    "        prompt = []\n",
    "        tok_count = 0\n",
    "        for msg in self._hist:\n",
    "            content = msg.content\n",
    "            is_last = msg == self._hist[-1]\n",
    "            if msg.response_format and is_last:\n",
    "                json_format = {k:'...' for k in msg.response_format}\n",
    "                content += f'\\n\\nRespond in this json: {json_format}'\n",
    "            elif msg.response_format:\n",
    "                content += '\\n\\nRespond in JSON.'\n",
    "            \n",
    "            tok_count += _ntoks(content)\n",
    "\n",
    "            prompt.append({\n",
    "                'role': msg.role,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        return prompt, tok_count\n",
    "\n",
    "    def _call_default(self, messages, temperature, max_tokens):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        out = out.choices[0].message.content\n",
    "        out_toks = _ntoks(out)\n",
    "\n",
    "        _inc_tok_count('out', out_toks)\n",
    "\n",
    "        self._hist.append(Msg('assistant', out))\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def _call_stream(self, messages, temperature, max_tokens):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        self._hist.append(Msg('assistant', ''))\n",
    "        self._awaiting_streamed = True\n",
    "\n",
    "        def tok_stream():\n",
    "            for t in out:\n",
    "                tok = t.choices[0].delta.content\n",
    "\n",
    "                if not tok: continue\n",
    "                _inc_tok_count('out', 1)\n",
    "                self._hist[-1].content += tok\n",
    "                yield tok\n",
    "\n",
    "            _inc_tok_count('out', 4) # 4 exta used in llama prompt format\n",
    "            self._awaiting_streamed = False\n",
    "\n",
    "        return tok_stream()\n",
    "\n",
    "    def _call_fmted(self, messages, temperature, max_tokens, response_format):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            response_format={'type': 'json_object'}   \n",
    "        )\n",
    "        out = out.choices[0].message.content\n",
    "        out_toks = _ntoks(out)\n",
    "        _inc_tok_count('out', out_toks)\n",
    "        \n",
    "        try:\n",
    "            out = json.loads(out)\n",
    "        except:\n",
    "            raise Exception(f'Bad JSON output. {out} != {resposne_format}')\n",
    "\n",
    "        if not all(k in out.keys() for k in response_format):\n",
    "            raise Exception(f'Missing json keys. {out.keys()} != {response_format}')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __call__(self, prompt, response_format:str|list|None=None, temperature=0, max_tokens=1024):\n",
    "        if self._awaiting_streamed:\n",
    "            raise Exception('Cannot start a new message before ending a streamed one.')\n",
    "\n",
    "        is_resp_fmted = type(response_format) is list\n",
    "\n",
    "        self._hist.append(Msg('user', prompt))\n",
    "        if is_resp_fmted:\n",
    "            self._hist[-1].response_format = response_format\n",
    "\n",
    "        messages, in_toks = self._hist_to_prompt()\n",
    "        _inc_tok_count('in', in_toks)\n",
    "\n",
    "        if response_format is None:\n",
    "            return self._call_default(messages, temperature, max_tokens)\n",
    "        elif response_format == 'stream':\n",
    "            return self._call_stream(messages, temperature, max_tokens)\n",
    "        elif is_resp_fmted:\n",
    "            return self._call_fmted(messages, temperature, max_tokens, response_format)\n",
    "        else:\n",
    "            raise Exception(f'Unsupported Response Format: {response_format}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot start a new message before ending a streamed one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m l(\u001b[39m'\u001b[39;49m\u001b[39mwho you\u001b[39;49m\u001b[39m'\u001b[39;49m, response_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt, response_format:\u001b[39mstr\u001b[39m\u001b[39m|\u001b[39m\u001b[39mlist\u001b[39m\u001b[39m|\u001b[39m\u001b[39mNone\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_awaiting_streamed:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot start a new message before ending a streamed one.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m     is_resp_fmted \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(response_format) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist\u001b[39m.\u001b[39mappend(Msg(\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, prompt))\n",
      "\u001b[0;31mException\u001b[0m: Cannot start a new message before ending a streamed one."
     ]
    }
   ],
   "source": [
    "out = l('who you', response_format='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Assistant', 'job': 'AI Conversational Model'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Msg(role='user', content='who you', response_format=None),\n",
       " Msg(role='assistant', content='', response_format=None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode('Hello! It seems like we\\'re having a friendly echo. How can I assist you today?').__len__() + 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
