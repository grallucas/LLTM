{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"correct_sentence\": \"Tämä on minun koirani\", \"breif explanation\": \"NA\"}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " "
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "    api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "out = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a Finnish teacher.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the most obvious mistake in this sentence (if it has one) \\\"Tämä on minun koirani\\\"? Respond in JSON {'correct_sentence': str, 'breif explanation': str|'NA'}.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    "    temperature=0,\n",
    "    response_format={'type': 'json_object'}\n",
    ")\n",
    "\n",
    "out_str = ''\n",
    "for t in out:\n",
    "    tok = t.choices[0].delta.content\n",
    "    if not tok: continue\n",
    "    print(tok, end='')\n",
    "    out_str += tok\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 2 (char 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m json\u001b[39m.\u001b[39;49mloads(out_str\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m}\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-conda-py312-glibc/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/data/ai_club/team_3_2024-25/team3-conda-py312-glibc/lib/python3.12/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 2 (char 84)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.loads(out_str+'}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chat-224c2af44cd046ba8a2f4706dc524330', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='In Finnish, the correct sentence would be \"Tämä on minun koirani\" actually has one mistake that is quite common for non-native speakers.\\n\\nThe mistake is the use of \"minun\" instead of \"omani\". \"Minun\" is the genitive form of the pronoun \"minä\", whereas \"omani\" is the possessive form.\\n\\nSo, the corrected sentence would be: \"Tämä on omani koira.\"\\n\\nHowever, it\\'s worth noting', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None), stop_reason=None)], created=1742499135, model='meta/llama-3.1-70b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=48, total_tokens=148, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a Finnish teacher.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the most obvious mistake in this sentence (if it has one) \\\"Tämä on minun koirani\\\"?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Finnish, the correct sentence would be \"Tämä on minun koirani\" actually has one mistake that is quite common for non-native speakers.\\n\\nThe mistake is the use of \"minun\" instead of \"omani\". \"Minun\" is the genitive form of the pronoun \"minä\", whereas \"omani\" is the possessive form.\\n\\nSo, the corrected sentence would be: \"Tämä on omani koira.\"\\n\\nHowever, it\\'s worth noting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/app/test_newllm.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m enc\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mYou are a Finnish teacher.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m \u001b[39m# 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "enc.encode(\"You are a Finnish teacher.\").__len__() + 4 # 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import getpass\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "USER = getpass.getuser()\n",
    "\n",
    "TOKEN_COUNT_PATH = None\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "    api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# TODO REMOVE\n",
    "TOKEN_COUNT_PATH = '/data/ai_club/team_3_2024-25/tokcounts2/'\n",
    "\n",
    "def _ntoks(text):\n",
    "    return enc.encode(text).__len__() + 4 # 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format)\n",
    "\n",
    "def _inc_tok_count(mode, amt):\n",
    "    if TOKEN_COUNT_PATH is None:\n",
    "        raise Exception('Set TOKEN_COUNT_PATH before infernece')\n",
    "    fname = f'{USER}_{mode}.txt'\n",
    "    try:\n",
    "        with open(TOKEN_COUNT_PATH+fname, 'r') as f:\n",
    "            count = int(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        count = 0\n",
    "    except ValueError:\n",
    "        raise Exception(f'Token Count Corrupted: {fname}')\n",
    "    \n",
    "    count += amt\n",
    "\n",
    "    with open(TOKEN_COUNT_PATH+fname, 'w') as f:\n",
    "        f.write(str(count)+'\\n')\n",
    "\n",
    "@dataclass\n",
    "class Msg:\n",
    "    role: str\n",
    "    content: str\n",
    "    response_format: list = None\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, sys_prompt:str=None):\n",
    "        self._hist = []\n",
    "        self._awaiting_streamed = False\n",
    "\n",
    "    def _hist_to_prompt(self):\n",
    "        prompt = []\n",
    "        tok_count = 0\n",
    "        for msg in self._hist:\n",
    "            content = msg.content\n",
    "            is_last = msg == self._hist[-1]\n",
    "            if msg.response_format and is_last:\n",
    "                json_format = {k:'...' for k in msg.response_format}\n",
    "                content += f'\\n\\nRespond in this json: {json_format}'\n",
    "            elif msg.response_format:\n",
    "                content += '\\n\\nRespond in JSON.'\n",
    "            \n",
    "            tok_count += _ntoks(content)\n",
    "\n",
    "            prompt.append({\n",
    "                'role': msg.role,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "        return prompt, tok_count\n",
    "\n",
    "    def _call_default(self, messages, temperature, max_tokens):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        out = out.choices[0].message.content\n",
    "        out_toks = _ntoks(out)\n",
    "\n",
    "        _inc_tok_count('out', out_toks)\n",
    "\n",
    "        self._hist.append(Msg('assistant', out))\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def _call_stream(self, messages, temperature, max_tokens):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        self._hist.append(Msg('assistant', ''))\n",
    "        self._awaiting_streamed = True\n",
    "\n",
    "        def tok_stream():\n",
    "            for t in out:\n",
    "                tok = t.choices[0].delta.content\n",
    "\n",
    "                if not tok: continue\n",
    "                _inc_tok_count('out', 1)\n",
    "                self._hist[-1].content += tok\n",
    "                yield tok\n",
    "\n",
    "            _inc_tok_count('out', 4) # 4 exta used in llama prompt format\n",
    "            self._awaiting_streamed = False\n",
    "\n",
    "        return tok_stream()\n",
    "\n",
    "    def _call_fmted(self, messages, temperature, max_tokens, response_format):\n",
    "        out = client.chat.completions.create(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            response_format={'type': 'json_object'}   \n",
    "        )\n",
    "        out = out.choices[0].message.content\n",
    "        out_toks = _ntoks(out)\n",
    "        _inc_tok_count('out', out_toks)\n",
    "        \n",
    "        try:\n",
    "            out = json.loads(out)\n",
    "        except:\n",
    "            raise Exception(f'Bad JSON output. {out} != {resposne_format}')\n",
    "\n",
    "        if not all(k in out.keys() for k in response_format):\n",
    "            raise Exception(f'Missing json keys. {out.keys()} != {response_format}')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __call__(self, prompt, response_format:str|list|None=None, temperature=0, max_tokens=1024):\n",
    "        if self._awaiting_streamed:\n",
    "            raise Exception('Cannot start a new message before ending a streamed one.')\n",
    "\n",
    "        is_resp_fmted = type(response_format) is list\n",
    "\n",
    "        self._hist.append(Msg('user', prompt))\n",
    "        if is_resp_fmted:\n",
    "            self._hist[-1].response_format = response_format\n",
    "\n",
    "        messages, in_toks = self._hist_to_prompt()\n",
    "        _inc_tok_count('in', in_toks)\n",
    "\n",
    "        if response_format is None:\n",
    "            return self._call_default(messages, temperature, max_tokens)\n",
    "        elif response_format == 'stream':\n",
    "            return self._call_stream(messages, temperature, max_tokens)\n",
    "        elif is_resp_fmted:\n",
    "            return self._call_fmted(messages, temperature, max_tokens, response_format)\n",
    "        else:\n",
    "            raise Exception(f'Unsupported Response Format: {response_format}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot start a new message before ending a streamed one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m l(\u001b[39m'\u001b[39;49m\u001b[39mwho you\u001b[39;49m\u001b[39m'\u001b[39;49m, response_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt, response_format:\u001b[39mstr\u001b[39m\u001b[39m|\u001b[39m\u001b[39mlist\u001b[39m\u001b[39m|\u001b[39m\u001b[39mNone\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_awaiting_streamed:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot start a new message before ending a streamed one.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m     is_resp_fmted \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(response_format) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/test/test_newllm.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist\u001b[39m.\u001b[39mappend(Msg(\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, prompt))\n",
      "\u001b[0;31mException\u001b[0m: Cannot start a new message before ending a streamed one."
     ]
    }
   ],
   "source": [
    "out = l('who you', response_format='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Assistant', 'job': 'AI Conversational Model'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Msg(role='user', content='who you', response_format=None),\n",
       " Msg(role='assistant', content='', response_format=None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode('Hello! It seems like we\\'re having a friendly echo. How can I assist you today?').__len__() + 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
