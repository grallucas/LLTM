{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"You are a Finnish teacher.\").__len__() + 4 # 4 extra tokens for llama: start_header, end_header, eos, 2 newlines (part of prompt format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"correct_sentence\": \"Tämä on minun koirani\", \"breif explanation\": \"NA\"}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "    api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "out = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a Finnish teacher.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the most obvious mistake in this sentence (if it has one) \\\"Tämä on minun koirani\\\"? Respond in JSON {'correct_sentence': str, 'breif explanation': str|'NA'}.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    "    temperature=0,\n",
    "    response_format={'type': 'json_object'}\n",
    ")\n",
    "\n",
    "for t in out:\n",
    "    tok = t.choices[0].delta.content\n",
    "    if not tok: continue\n",
    "    print(tok, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_core.llm as L\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_toks = L.LLM_GLOBAL_INSTANCE.tokenize(\n",
    "'''\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a Finnish language teacher.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What are the mistakes in \"Tämä on minun koira\"? Give me: [correct version] [breif list of mistakes].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''.strip().encode()+b'\\n\\n', special=True, add_bos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE.token_bos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 271]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE.tokenize(b'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78191]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE.tokenize(b'assistant', special=True, add_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE.token_eos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [L.LLM_GLOBAL_INSTANCE.token_bos()]\n",
    "\n",
    "newlines = L.LLM_GLOBAL_INSTANCE.tokenize(b'\\n\\n', special=True, add_bos=False)[0]\n",
    "start_header = L.LLM_GLOBAL_INSTANCE.tokenize(b'<|start_header_id|>', special=True, add_bos=False)[0]\n",
    "end_header = L.LLM_GLOBAL_INSTANCE.tokenize(b'<|end_header_id|>', special=True, add_bos=False)[0]\n",
    "eos = L.LLM_GLOBAL_INSTANCE.token_eos()\n",
    "\n",
    "role = {\n",
    "    name: L.LLM_GLOBAL_INSTANCE.tokenize(name.encode(), special=True, add_bos=False)[0]\n",
    "    for name in ['system', 'user', 'assistant']\n",
    "}\n",
    "\n",
    "for msg in llm._hist:\n",
    "    msg_content = msg.content\n",
    "    is_last = msg == llm._hist[-1]\n",
    "    has_fmt = not not msg.response_format\n",
    "    # if has_fmt and is_last:\n",
    "    #     msg_content += f'\\n\\n\\n{response_fmt.pretty_response_format(msg.response_format)}'\n",
    "    # if has_fmt and not is_last:\n",
    "    #     msg_content += '\\n\\n\\nRespond in JSON.'\n",
    "\n",
    "    tokenized_message = L.LLM_GLOBAL_INSTANCE.tokenize(msg_content.encode(), special=False, add_bos=False)\n",
    "\n",
    "    prompt += [start_header, role[msg.role], end_header, newlines, *tokenized_message, eos]\n",
    "\n",
    "    if is_last and msg.role == 'user':\n",
    "        prompt += [start_header, role['assistant'], end_header, newlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 8 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     586.73 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7109.11 ms /    46 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-fedc1d3d-8d85-4935-8dbf-cc8cb4e49847',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1742430273,\n",
       " 'model': '/data/ai_club/llms/Llama-3.3-70B-Instruct-Q6_K_L-00001-of-00002.gguf',\n",
       " 'choices': [{'text': 'Here are a few fundamental mathematical constants:\\n\\n1. **Pi (π)**: approximately 3.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 20, 'total_tokens': 55}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE(\n",
    "    prompt,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 50 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     586.73 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7320.70 ms /    21 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-660fdb24-90fa-49d4-9c90-2b78d9d006db',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1742429438,\n",
       " 'model': '/data/ai_club/llms/Llama-3.3-70B-Instruct-Q6_K_L-00001-of-00002.gguf',\n",
       " 'choices': [{'text': 'Tämä on minun koirani \\n* \"koira\" should be \"koir',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 51, 'completion_tokens': 20, 'total_tokens': 71}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.LLM_GLOBAL_INSTANCE(input_toks, max_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Global LLM Instance\n"
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a Finnish teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm('Wazzup!!!!!', response_format='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hei! Mitä kuuluu? (Hello! How are you?) Let's try to speak some Finnish, shall we? What does \"Wazzup\" mean to you, and how can we translate it into Finnish? Maybe we can say \"Mitä häätä\" or \"Moi moi\"?"
     ]
    }
   ],
   "source": [
    "for t in out:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mLLM(\u001b[39m'\u001b[39m\u001b[39mYou are a statistic and fact sharing bot.\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m s \u001b[39m=\u001b[39m llm(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mHello! Give me a few mathematial constants?\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mconstants\u001b[39;49m\u001b[39m'\u001b[39;49m: [{\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mfloat\u001b[39;49m}, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     },\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# for t in s:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     print(t, end='')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/lltm/LLTM/llm_core/test_llm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/Documents/aiClub/lltm/LLTM/llm_core/llm.py:189\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, msg, response_format, temperature, max_tokens, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m# It's possible to have a non-dict response_format with grammars, but this remains unimplemented \u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist\u001b[39m.\u001b[39mappend(Msg(\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, msg, (response_format \u001b[39mif\u001b[39;00m response_format_is_special \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)))\n\u001b[0;32m--> 189\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hist_to_prompt()\n\u001b[1;32m    190\u001b[0m _inc_tok_count(\u001b[39m'\u001b[39m\u001b[39min\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(prompt))\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m response_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/aiClub/lltm/LLTM/llm_core/llm.py:155\u001b[0m, in \u001b[0;36mLLM._hist_to_prompt\u001b[0;34m(self, inject_resp)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmixtral\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m MODEL_PATH: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist_to_prompt_mixtral(inject_resp)\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mqwen\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m MODEL_PATH: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist_to_prompt_qwen(inject_resp)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mLlama\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m MODEL_PATH: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hist_to_prompt_llama(inject_resp)\n\u001b[1;32m    156\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel type not supported\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/aiClub/lltm/LLTM/llm_core/llm.py:150\u001b[0m, in \u001b[0;36mLLM._hist_to_prompt_llama\u001b[0;34m(self, inject_resp)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_hist_to_prompt_llama\u001b[39m(\u001b[39mself\u001b[39m, inject_resp):\n\u001b[0;32m--> 150\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! Give me a few mathematial constants?',\n",
    "    response_format={\n",
    "        'constants': [{'name': str, 'value': float}, ...]\n",
    "    },\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respond in JSON with this format, writing strings with double quotes and numbers with JS notation (commas end rather than separate digits): {\"country\": str, \"cities\": [{\"city_name\": str, \"population\": int}, 10 total...]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Generating {'country': <class 'str'>, 'cities': [{'city_name': <class 'str'>, 'population': <class 'int'>}, 10]}\n",
      "Generating <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ai_club/team_3_2024-25/team3-env-py312-glibc/lib/python3.12/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: \"United States\"\n",
      "Generating [{'city_name': <class 'str'>, 'population': <class 'int'>}, 10]\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Milwaukee\"\n",
      "Generating <class 'int'>\n",
      "Generated: 595351\n",
      "Generated: {\"city_name\": \"Milwaukee\", \"population\": 595351}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Madison\"\n",
      "Generating <class 'int'>\n",
      "Generated: 255214\n",
      "Generated: {\"city_name\": \"Madison\", \"population\": 255214}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Green Bay\"\n",
      "Generating <class 'int'>\n",
      "Generated: 104779\n",
      "Generated: {\"city_name\": \"Green Bay\", \"population\": 104779}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Kenosha\"\n",
      "Generating <class 'int'>\n",
      "Generated: 99218\n",
      "Generated: {\"city_name\": \"Kenosha\", \"population\": 99218}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Racine\"\n",
      "Generating <class 'int'>\n",
      "Generated: 77122\n",
      "Generated: {\"city_name\": \"Racine\", \"population\": 77122}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Appleton\"\n",
      "Generating <class 'int'>\n",
      "Generated: 74091\n",
      "Generated: {\"city_name\": \"Appleton\", \"population\": 74091}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Waukesha\"\n",
      "Generating <class 'int'>\n",
      "Generated: 72325\n",
      "Generated: {\"city_name\": \"Waukesha\", \"population\": 72325}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Eau Claire\"\n",
      "Generating <class 'int'>\n",
      "Generated: 68733\n",
      "Generated: {\"city_name\": \"Eau Claire\", \"population\": 68733}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Oshkosh\"\n",
      "Generating <class 'int'>\n",
      "Generated: 66342\n",
      "Generated: {\"city_name\": \"Oshkosh\", \"population\": 66342}\n",
      "Generating {'city_name': <class 'str'>, 'population': <class 'int'>}\n",
      "Generating <class 'str'>\n",
      "Generated: \"Janesville\"\n",
      "Generating <class 'int'>\n",
      "Generated: 65743\n",
      "Generated: {\"city_name\": \"Janesville\", \"population\": 65743}\n",
      "Generated: [{\"city_name\": \"Milwaukee\", \"population\": 595351}, {\"city_name\": \"Madison\", \"population\": 255214}, {\"city_name\": \"Green Bay\", \"population\": 104779}, {\"city_name\": \"Kenosha\", \"population\": 99218}, {\"city_name\": \"Racine\", \"population\": 77122}, {\"city_name\": \"Appleton\", \"population\": 74091}, {\"city_name\": \"Waukesha\", \"population\": 72325}, {\"city_name\": \"Eau Claire\", \"population\": 68733}, {\"city_name\": \"Oshkosh\", \"population\": 66342}, {\"city_name\": \"Janesville\", \"population\": 65743}]\n",
      "Generated: {\"country\": \"United States\", \"cities\": [{\"city_name\": \"Milwaukee\", \"population\": 595351}, {\"city_name\": \"Madison\", \"population\": 255214}, {\"city_name\": \"Green Bay\", \"population\": 104779}, {\"city_name\": \"Kenosha\", \"population\": 99218}, {\"city_name\": \"Racine\", \"population\": 77122}, {\"city_name\": \"Appleton\", \"population\": 74091}, {\"city_name\": \"Waukesha\", \"population\": 72325}, {\"city_name\": \"Eau Claire\", \"population\": 68733}, {\"city_name\": \"Oshkosh\", \"population\": 66342}, {\"city_name\": \"Janesville\", \"population\": 65743}]}\n",
      "{'country': 'United States', 'cities': [{'city_name': 'Milwaukee', 'population': 595351}, {'city_name': 'Madison', 'population': 255214}, {'city_name': 'Green Bay', 'population': 104779}, {'city_name': 'Kenosha', 'population': 99218}, {'city_name': 'Racine', 'population': 77122}, {'city_name': 'Appleton', 'population': 74091}, {'city_name': 'Waukesha', 'population': 72325}, {'city_name': 'Eau Claire', 'population': 68733}, {'city_name': 'Oshkosh', 'population': 66342}, {'city_name': 'Janesville', 'population': 65743}]}\n"
     ]
    }
   ],
   "source": [
    "llm = L.LLM('You are a statistic and fact sharing bot.')\n",
    "\n",
    "s = llm(\n",
    "    'Hello! What are some Wisconsin cities?',\n",
    "    response_format={'country': str, 'cities': [{'city_name': str, 'population': int}, 10]},\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# for t in s:\n",
    "#     print(t, end='')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['cities'][2]['city_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.LLM.detokenize(llm._hist_to_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class Repr:\n",
    "    def __init__(self, s): self.s = s\n",
    "    def __str__(self): return self.s\n",
    "    def __repr__(self): return self.s\n",
    "\n",
    "def pretty_response_format(response_format):\n",
    "    if response_format in [str, int, float]:\n",
    "        return Repr(response_format.__name__)\n",
    "    elif type(response_format) == dict:\n",
    "        return {\n",
    "            Repr(f'\"{k}\"'): pretty_response_format(v)\n",
    "            for k,v in response_format.items()\n",
    "        }\n",
    "    elif type(response_format) == list:\n",
    "        sub, etc = response_format\n",
    "\n",
    "        if etc == ...:\n",
    "            return [pretty_response_format(sub), Repr('...')]\n",
    "        elif type(etc) is int:\n",
    "            return [pretty_response_format(sub), Repr(f'{etc} total...')]\n",
    "\n",
    "        else:\n",
    "            raise Exception(f'List size must be int or `...`, but found: {etc}')\n",
    "\n",
    "    else:\n",
    "        raise Exception(f'Unsupported value type: {v}')\n",
    "\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'local_names': [str, ...]}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_country': {'name': str, 'pop': int}}\n",
    "# fmt = {'country': str, 'capital': str, 'pop': int, 'nearest_cities': [{'name': str, 'pop': int}, 3]}\n",
    "fmt = str\n",
    "\n",
    "fmt_prompt = pretty_response_format(fmt)\n",
    "\n",
    "# print(str(fmt_prompt))\n",
    "# print(f'{fmt_prompt}')\n",
    "# assert 0\n",
    "\n",
    "def gen_response_formated_rec(response_format, prompt_ref, temperature=0, max_tokens=1000):\n",
    "    if response_format == str:\n",
    "        prompt_ref[0] += '\"'\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=str_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span(2)[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "        prompt_ref[0] += '\"'\n",
    "\n",
    "    elif response_format in [float, int]:\n",
    "        s = L.LLM_GLOBAL_INSTANCE(\n",
    "            prompt_ref[0],\n",
    "            max_tokens=max_tokens, temperature=temperature,\n",
    "            stream=True,\n",
    "            # grammar=num_grammar\n",
    "        )\n",
    "        for t in s:\n",
    "            t = t['choices'][0]['text']\n",
    "            str_end = re.search(r'[,}]', t)\n",
    "            if str_end:\n",
    "                prompt_ref[0] += t[:str_end.span()[0]]\n",
    "                break\n",
    "            prompt_ref[0] += t\n",
    "\n",
    "    elif type(response_format) == dict:\n",
    "        prompt_ref[0] += '{'\n",
    "        first = True\n",
    "        for k,v in response_format.items():\n",
    "            if not first: prompt_ref[0] += ', '\n",
    "            first = False\n",
    "            prompt_ref[0] += f'\"{k}\": '\n",
    "            gen_response_formated_rec(v, prompt_ref, temperature, max_tokens)\n",
    "        prompt_ref[0] += '}'\n",
    "\n",
    "    elif type(response_format) == list:\n",
    "        sub_type, etc = response_format\n",
    "\n",
    "        i = None\n",
    "        if type(etc) is int:\n",
    "            i = etc\n",
    "        else:\n",
    "            assert etc == ...\n",
    "\n",
    "        prompt_ref[0] += '['\n",
    "        while True:\n",
    "            if not i is None:\n",
    "                if i == 0: break\n",
    "                i -= 1\n",
    "\n",
    "            gen_response_formated_rec(sub_type, prompt_ref, temperature, max_tokens)\n",
    "            next_tok = L.LLM_GLOBAL_INSTANCE(\n",
    "                prompt_ref[0],\n",
    "                max_tokens=1, temperature=0,\n",
    "                stream=False,\n",
    "            )['choices'][0]['text'].strip()\n",
    "            \n",
    "            if ']' in next_tok:\n",
    "                break\n",
    "            elif ',' in next_tok:\n",
    "                prompt_ref[0] += ', '\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f'Theoretically unreachable err: invalid list next tok: {next_tok}')\n",
    "\n",
    "        prompt_ref[0] += ']'\n",
    "\n",
    "def gen_response_formated(response_format, prompt, temperature=0, max_tokens=1000):\n",
    "    json_start = len(prompt)\n",
    "    prompt_ref = [prompt]\n",
    "    gen_response_formated_rec(fmt, prompt_ref, temperature, max_tokens)\n",
    "    return prompt_ref[0][json_start:]\n",
    "\n",
    "prompt = f'[INST] What is one capital city in Europe? Respond in JSON with this format ending strings with double quotes and numbers with JS notation (commas end rather than separate digits): {fmt_prompt} [/INST] '\n",
    "print(prompt)\n",
    "\n",
    "resp = gen_response_formated(fmt, prompt)\n",
    "\n",
    "print(resp)\n",
    "\n",
    "# assert 0\n",
    "\n",
    "# json_start = len(prompt)\n",
    "# prompt += '{'\n",
    "\n",
    "\n",
    "# first = True\n",
    "# for k,v in fmt.items():\n",
    "#     if not first: prompt += ', '\n",
    "#     first = False\n",
    "#     prompt += f'\"{k}\": '\n",
    "#     if v == str:\n",
    "#         prompt += '\"'\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=str_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'(^|[^\\\\])(\")', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span(2)[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#         prompt += '\"'\n",
    "#     elif v in [int, float]:\n",
    "#         s = L.LLM_GLOBAL_INSTANCE(\n",
    "#             prompt,\n",
    "#             max_tokens=1000, temperature=0,\n",
    "#             stream=True,\n",
    "#             grammar=num_grammar\n",
    "#         )\n",
    "#         for t in s:\n",
    "#             t = t['choices'][0]['text']\n",
    "#             str_end = re.search(r'[,}]', t)\n",
    "#             if str_end:\n",
    "#                 prompt += t[:str_end.span()[0]]\n",
    "#                 break\n",
    "#             prompt += t\n",
    "#     else:\n",
    "#         raise Exception(f\"Unsupported value type: {v}\")\n",
    "# prompt += '}'\n",
    "\n",
    "# resp = json.loads(prompt[json_start:])\n",
    "\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    json.loads('owegboieqob')\n",
    "except json.JSONDecodeError as e:\n",
    "    raise Exception(f'Cannot parse this json: 23223542')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = L.LLM('You are a Finnish language teacher teaching Finnish to me through conversation. You must give accurate information.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Oletko koira? respond in English.', response_format='stream', max_tokens=1000, temperature=0.1)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Why did you start your response with an extra space????? AHHHH!!', response_format='stream', max_tokens=1000, temperature=0)\n",
    "for t in s:\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm._hist_to_prompt()\n",
    "L.LLM.detokenize(llm._hist_to_prompt())\n",
    "# print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = L.LLM_GLOBAL_INSTANCE.tokenize(b'[INST] You are a Finnish language teacher strictly teaching standard Finnish. You must give accurate information. [/INST] Understood.', add_bos=False)\n",
    "ts = [1, *ts, 2]\n",
    "ts += L.LLM_GLOBAL_INSTANCE.tokenize(' [INST] What does \"mitä on sinun nimi\" mean? 1) What is another way to say it? 2) Why is the original version NOT grammatically correct? Tell me in terms of every individual word. [/INST] '.encode('utf-8'), add_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L.LLM.detokenize(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = L.LLM_GLOBAL_INSTANCE(ts, temperature=0, max_tokens=1000, stream=True)\n",
    "for tok in resp:\n",
    "    print(tok['choices'][0]['text'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm('Hello. How are yo?', response_format={'my_name':'...', 'my_response':'...'}, max_tokens=1000)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L.LLM.detokenize(llm._hist_to_prompt()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$$  TODO ^ remove the \"respond in json\" instruction in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_processor(prev_tok_ids, next_tok_logits):\n",
    "    \n",
    "    # print(prev_tok_ids, next_tok_logits)\n",
    "\n",
    "    next_tok_logits[995] -= 1000\n",
    " \n",
    "    return next_tok_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = L.LLM('You are a British butler. Start every answer with \"you\"')\n",
    "llm = L.LLM('You are a British assistant.')\n",
    "resp = llm('Hello. What is the capital of Finland?', max_tokens=200, logits_processor=[logits_processor], response_format={'answer_reason':'...', 'answer':'...'})\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.get_pretty_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.tokenize(' You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.detokenize([1, 995])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
